---
title: Dostawcy
description: Korzystanie z dowolnego dostawcy LLM w opencode.
---

import config from "../../../../config.mjs"
export const console = config.console

opencode używa [AI SDK](https://ai-sdk.dev/) i [Models.dev](https://models.dev) do obsługi **ponad 75 dostawców LLM** i obsługuje uruchamianie modeli lokalnych.

Aby dodać dostawcę należy:

1. Dodaj klucze API dla dostawcy za pomocą komendy `/connect`.
2. Skonfiguruj dostawcę w konfiguracji opencode.

---

### Credentials

Po dodaniu kluczy API dostawcy za pomocą polecenia `/connect` są one przechowywane
w `~/.local/share/opencode/auth.json`.

---

### Konfiguracja

Możesz dostosować dostawców za pomocą sekcji `provider` w swoim opencode
config.

---

#### Base URL

Możesz dostosować podstawowy adres URL dla dowolnego dostawcy, ustawiając opcję `baseURL`. Jest to przydatne podczas korzystania z usług proxy lub niestandardowych punktów końcowych.

```json title="opencode.json" {6}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "anthropic": {
      "options": {
        "baseURL": "https://api.anthropic.com/v1"
      }
    }
  }
}
```

---

## OpenCode Zen

OpenCode Zen to lista modeli dostarczonych przez zespół opencode, które zostały
przetestowane i zweryfikowane, aby dobrze współpracować z opencode. [Dowiedz się więcej](/docs/zen).

:::tip
Jeśli jesteś nowy, zalecamy rozpoczęcie od OpenCode Zen.
:::

1. Uruchom polecenie `/connect` w TUI, wybierz opencode i przejdź do [opencode.ai/auth](https://opencode.ai/auth).

   ```txt
   /connect
   ```

2. Zaloguj się, dodaj szczegóły rozliczeniowe i skopiuj klucz API.

3. Wklej swój klucz API.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom `/models` w TUI, aby zobaczyć listę zalecanych przez nas modeli.

   ```txt
   /models
   ```

Działa jak każdy inny dostawca w opencode i jest całkowicie opcjonalny w użyciu.

---

## Katalog

Przyjrzyjmy się szczegółowo niektórym dostawcom. Jeśli chcesz dodać dostawcę do
listę, możesz otworzyć PR.

:::note
Nie widzisz tutaj dostawcy? Prześlij PR.
:::

---

### 302.AI

1. Przejdź do [konsoli 302.AI](https://302.ai/), utwórz konto i wygeneruj klucz API.

2. Uruchom komendę `/connect` i wyszukaj **302.AI**.

   ```txt
   /connect
   ```

3. Wpisz swój klucz API 302.AI.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model.

   ```txt
   /models
   ```

---

### Amazon Bedrock

Aby używać Amazon Bedrock z opencode:

1. Przejdź do **Katalogu modeli** w konsoli Amazon Bedrock i poproś
   dostęp do wybranych modeli.

   :::tip
   Musisz mieć dostęp do żądanego modelu w Amazon Bedrock.
   :::

2. **Skonfiguruj uwierzytelnianie** przy użyciu jednej z następujących metod:

   #### Zmienne środowiskowe (Szybki start)

   Ustaw jedną z tych zmiennych środowiskowych podczas uruchamiania opencode:

   ```bash
   # Option 1: Using AWS access keys
   AWS_ACCESS_KEY_ID=XXX AWS_SECRET_ACCESS_KEY=YYY opencode

   # Option 2: Using named AWS profile
   AWS_PROFILE=my-profile opencode

   # Option 3: Using Bedrock bearer token
   AWS_BEARER_TOKEN_BEDROCK=XXX opencode
   ```

   Lub dodaj je do swojego profilu bash:

   ```bash title="~/.bash_profile"
   export AWS_PROFILE=my-dev-profile
   export AWS_REGION=us-east-1
   ```

   #### Plik konfiguracyjny (zalecane)

   W przypadku konfiguracji specyficznej dla projektu lub trwałej użyj `opencode.json`:

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "my-aws-profile"
         }
       }
     }
   }
   ```

   **Dostępne opcje:**
   - `region` - region AWS (np. `us-east-1`, `eu-west-1`)
   - `profile` - profil nazwany AWS z `~/.aws/credentials`
   - `endpoint` — niestandardowy adres URL punktu końcowego dla punktów końcowych VPC (alias dla ogólnej opcji `baseURL`)

   :::tip
   Opcje z pliku konfiguracyjnego mają pierwszeństwo przed zmiennymi środowiskowymi.
   :::

   #### Zaawansowane: Punkty końcowe VPC

   Jeśli używasz punktów końcowych VPC dla Bedrock:

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "production",
           "endpoint": "https://bedrock-runtime.us-east-1.vpce-xxxxx.amazonaws.com"
         }
       }
     }
   }
   ```

   :::note
   Opcja `endpoint` jest aliasem ogólnej opcji `baseURL`, używając terminologii specyficznej dla AWS. Jeśli określono zarówno `endpoint`, jak i `baseURL`, pierwszeństwo ma `endpoint`.
   :::

   #### Metody uwierzytelniania
   - **`AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`**: Utwórz użytkownika IAM i wygeneruj klucze dostępu w konsoli AWS
   - **`AWS_PROFILE`**: Użyj nazwanych profili z `~/.aws/credentials`. Najpierw skonfiguruj za pomocą `aws configure --profile my-profile` lub `aws sso login`
   - **`AWS_BEARER_TOKEN_BEDROCK`**: Wygeneruj długoterminowe klucze API z konsoli Amazon Bedrock
   - **`AWS_WEB_IDENTITY_TOKEN_FILE` / `AWS_ROLE_ARN`**: Dla EKS IRSA (Role IAM dla kont usług) lub innych środowisk Kubernetes z federacją OIDC. Te zmienne środowiskowe są automatycznie wstrzykiwane przez Kubernetes podczas korzystania z adnotacji konta usługi.

   #### Kolejność uwierzytelniania

   Amazon Bedrock wykorzystuje następujący priorytet uwierzytelniania:
   1. **Token nośnika** - zmienna środowiskowa `AWS_BEARER_TOKEN_BEDROCK` lub token z komendy `/connect`
   2. **AWS Credential Chain** - Profile, access keys, shared credentials, IAM roles, Web Identity Tokens (EKS IRSA), instance metadata

   :::note
   Gdy ustawisz bearer token (przez `/connect` lub `AWS_BEARER_TOKEN_BEDROCK`), ma on pierwszeństwo nad wszystkimi metodami poświadczeń AWS, w tym profilami.
   :::

3. Uruchom komendę `/models`, aby wybrać żądany model.

   ```txt
   /models
   ```

:::note
W przypadku niestandardowych profili wnioskowania użyj nazwy modelu i dostawcy w kluczu i ustaw właściwość `id` na wartość arn. Zapewnia to prawidłowe buforowanie:

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "amazon-bedrock": {
      // ...
      "models": {
        "anthropic-claude-sonnet-4.5": {
          "id": "arn:aws:bedrock:us-east-1:xxx:application-inference-profile/yyy"
        }
      }
    }
  }
}
```

:::

---

### Anthropic

1. Po zarejestrowaniu się uruchom komendę `/connect` i wybierz opcję Anthropic.

   ```txt
   /connect
   ```

2. Tutaj możesz wybrać opcję **Claude Pro/Max**, co spowoduje otwarcie przeglądarki
   i poproś o uwierzytelnienie.

   ```txt
   ┌ Select auth method
   │
   │ Claude Pro/Max
   │ Create an API Key
   │ Manually enter API Key
   └
   ```

3. Teraz wszystkie modele antropiczne powinny być dostępne po użyciu polecenia `/models`.

   ```txt
   /models
   ```

:::info
Korzystanie z subskrypcji Claude Pro/Max w opencode nie jest oficjalnie obsługiwane przez [Anthropic](https://anthropic.com).
:::

##### Użycie kluczy API

Możesz także wybrać opcję **Utwórz klucz API**, jeśli nie masz subskrypcji Pro/Max. Otworzy się także Twoja przeglądarka i poprosi Cię o zalogowanie się do Anthropic i poda kod, który możesz wkleić w terminalu.

Lub jeśli masz już klucz API, możesz wybrać **Wprowadź klucz API ręcznie** i wkleić go w terminalu.

---

### Azure OpenAI

:::note
Jeśli napotkasz błędy „Przykro mi, ale nie mogę pomóc w tej prośbie”, spróbuj zmienić filtr zawartości z **DefaultV2** na **Default** w zasobie platformy Azure.
:::

1. Przejdź do [Azure portal](https://portal.azure.com/) i utwórz zasób **Azure OpenAI**. Będziesz potrzebować:
   - **Nazwa zasobu**: staje się częścią punktu końcowego API (`https://RESOURCE_NAME.openai.azure.com/`)
   - **Klucz API**: `KEY 1` lub `KEY 2` z Twojego zasobu

2. Przejdź do [Azure AI Foundry](https://ai.azure.com/) i wdróż model.

   :::note
   Aby kod opencode działał poprawnie, nazwa wdrożenia musi być zgodna z nazwą modelu.
   :::

3. Uruchom polecenie `/connect` i wyszukaj **Azure**.

   ```txt
   /connect
   ```

4. Wpisz swój klucz API.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. Ustaw nazwę zasobu jako zmienną środowiskową:

   ```bash
   AZURE_RESOURCE_NAME=XXX opencode
   ```

   Lub dodaj go do swojego profilu bash:

   ```bash title="~/.bash_profile"
   export AZURE_RESOURCE_NAME=XXX
   ```

6. Uruchom komendę `/models`, aby wybrać wdrożony model.

   ```txt
   /models
   ```

---

### Azure Cognitive Services

1. Przejdź do [Azure portal](https://portal.azure.com/) i utwórz zasób **Azure OpenAI**. Będziesz potrzebować:
   - **Nazwa zasobu**: staje się częścią punktu końcowego API (`https://AZURE_COGNITIVE_SERVICES_RESOURCE_NAME.cognitiveservices.azure.com/`)
   - **Klucz API**: `KEY 1` lub `KEY 2` z Twojego zasobu

2. Przejdź do [Azure AI Foundry](https://ai.azure.com/) i wdróż model.

   :::note
   Aby kod opencode działał poprawnie, nazwa wdrożenia musi być zgodna z nazwą modelu.
   :::

3. Uruchom polecenie `/connect` i wyszukaj **Azure Cognitive Services**.

   ```txt
   /connect
   ```

4. Wpisz swój klucz API.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. Ustaw nazwę zasobu jako zmienną środowiskową:

   ```bash
   AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX opencode
   ```

   Lub dodaj go do swojego profilu bash:

   ```bash title="~/.bash_profile"
   export AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX
   ```

6. Uruchom komendę `/models`, aby wybrać wdrożony model.

   ```txt
   /models
   ```

---

### Baseten

1. Udaj się do [Baseten](https://app.baseten.co/), utwórz konto i wygeneruj klucz API.

2. Uruchom komendę `/connect` i wyszukaj **Baseten**.

   ```txt
   /connect
   ```

3. Wprowadź swój klucz API Baseten.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model.

   ```txt
   /models
   ```

---

### Cerebras

1. Przejdź do [konsoli Cerebras](https://inference.cerebras.ai/), utwórz konto i wygeneruj klucz API.

2. Uruchom komendę `/connect` i wyszukaj **Cerebras**.

   ```txt
   /connect
   ```

3. Wprowadź swój klucz API Cerebras.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Qwen 3 Coder 480B_.

   ```txt
   /models
   ```

---

### Cloudflare AI Gateway

Cloudflare AI Gateway umożliwia dostęp do modeli z OpenAI, Anthropic, Workers AI i innych za pośrednictwem ujednoliconego punktu końcowego. Dzięki [Ujednoliconemu rozliczeniu](https://developers.cloudflare.com/ai-gateway/features/unified-billing/) nie potrzebujesz oddzielnych kluczy API dla każdego dostawcy.

1. Przejdź do [panelu Cloudflare](https://dash.cloudflare.com/), przejdź do **AI** > **AI Gateway** i utwórz nową bramę.

2. Ustaw identyfikator konta i identyfikator bramy jako zmienne środowiskowe.

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_ACCOUNT_ID=your-32-character-account-id
   export CLOUDFLARE_GATEWAY_ID=your-gateway-id
   ```

3. Uruchom polecenie `/connect` i wyszukaj **Cloudflare AI Gateway**.

   ```txt
   /connect
   ```

4. Wprowadź swój token API Cloudflare.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

   Or set it as an environment variable.

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_API_TOKEN=your-api-token
   ```

5. Uruchom komendę `/models`, aby wybrać model.

   ```txt
   /models
   ```

   Możesz także dodawać modele za pomocą konfiguracji opencode.

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "cloudflare-ai-gateway": {
         "models": {
           "openai/gpt-4o": {},
           "anthropic/claude-sonnet-4": {}
         }
       }
     }
   }
   ```

---

### Cortecs

1. Przejdź do [konsoli Cortecs](https://cortecs.ai/), utwórz konto i wygeneruj klucz API.

2. Uruchom komendę `/connect` i wyszukaj **Cortecs**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API Cortecs.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Kimi K2 Instruct_.

   ```txt
   /models
   ```

---

### DeepSeek

1. Przejdź do [konsoli DeepSeek](https://platform.deepseek.com/), utwórz konto i kliknij **Utwórz nowy klucz API**.

2. Uruchom polecenie `/connect` i wyszukaj **DeepSeek**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API DeepSeek.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model DeepSeek, np. _DeepSeek Reasoner_.

   ```txt
   /models
   ```

---

### Deep Infra

1. Przejdź do [panelu Deep Infra](https://deepinfra.com/dash), utwórz konto i wygeneruj klucz API.

2. Uruchom komendę `/connect` i wyszukaj **Deep Infra**.

   ```txt
   /connect
   ```

3. Wprowadź swój klucz API Deep Infra.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model.

   ```txt
   /models
   ```

---

### Firmware

1. Przejdź do [Firmware dashboard](https://app.firmware.ai/signup), utwórz konto i wygeneruj klucz API.

2. Uruchom polecenie `/connect` i wyszukaj **Firmware**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API Firmware.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model.

   ```txt
   /models
   ```

---

### Fireworks AI

1. Przejdź do [konsoli Fireworks AI](https://app.fireworks.ai/), utwórz konto i kliknij **Utwórz klucz API**.

2. Uruchom polecenie `/connect` i wyszukaj **Fireworks AI**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API programu Fireworks AI.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Kimi K2 Instruct_.

   ```txt
   /models
   ```

---

### GitLab Duo

GitLab Duo zapewnia czat agentowy oparty na sztucznej inteligencji z natywnymi możliwościami wywoływania narzędzi za pośrednictwem Anthropic proxy GitLab.

1. Uruchom komendę `/connect` i wybierz GitLab.

   ```txt
   /connect
   ```

2. Wybierz metodę uwierzytelniania:

   ```txt
   ┌ Select auth method
   │
   │ OAuth (Recommended)
   │ Personal Access Token
   └
   ```

   #### Using OAuth (Recommended)

   Wybierz **OAuth**, a Twoja przeglądarka otworzy się w celu autoryzacji.

   #### Użycie osobistego tokenu dostępu
   1. Przejdź do [Ustawienia użytkownika GitLab > Tokeny dostępu](https://gitlab.com/-/user_settings/personal_access_tokens)
   2. Click **Add new token**
   3. Name: `OpenCode`, Scopes: `api`
   4. Skopiuj token (zaczyna się od `glpat-`)
   5. Wpisz go w terminalu

3. Uruchom komendę `/models`, aby zobaczyć dostępne modele.

   ```txt
   /models
   ```

   Dostępne są trzy modele oparte na Claude:
   - **duo-chat-haiku-4-5** (Domyślnie) - Szybkie odpowiedzi dla szybkich zadań
   - **duo-chat-sonnet-4-5** - Zrównoważona wydajność dla większości przepływów pracy
   - **duo-chat-opus-4-5** - Najbardziej zdolny do złożonej analizy

:::note
Jeśli nie chcesz, możesz także określić zmienną środowiskową „GITLAB_TOKEN”.
to store token in opencode auth storage.
:::

##### Self-Hosted GitLab

:::note[compliance note]
opencode używa małego modelu do niektórych zadań AI, takich jak generowanie tytułu sesji.
Domyślnie jest skonfigurowany do używania gpt-5-nano, hostowanego przez Zen. Aby zablokować opencode
aby używać tylko własnej instancji hostowanej przez GitLab, dodaj następujące elementy do pliku
`opencode.json` plik. Zalecane jest również wyłączenie udostępniania sesji.

```json
{
  "$schema": "https://opencode.ai/config.json",
  "small_model": "gitlab/duo-chat-haiku-4-5",
  "share": "disabled"
}
```

:::

W przypadku samodzielnie hostowanych instancji GitLab:

```bash
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_TOKEN=glpat-...
```

Jeśli w Twojej instancji działa niestandardowa brama AI:

```bash
GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
```

Lub dodaj do swojego profilu bash:

```bash title="~/.bash_profile"
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
export GITLAB_TOKEN=glpat-...
```

:::note
Twój administrator GitLab musi włączyć następujące opcje:

1. [Platforma Duo Agent](https://docs.gitlab.com/user/gitlab_duo/turn_on_off/) dla użytkownika, grupy lub instancji
2. Feature flags (via Rails console):
   - `agent_platform_claude_code`
   - `third_party_agents_enabled`
     :::

##### OAuth dla instancji hostowanych samodzielnie

Aby Oauth mógł działać w przypadku Twojej instancji hostowanej samodzielnie, musisz utworzyć
nową aplikację (Ustawienia → Aplikacje) z
Adres URL wywołania zwrotnego `http://127.0.0.1:8080/callback` i następujące zakresy:

- api (Uzyskaj dostęp do API w swoim imieniu)
- read_user (Przeczytaj swoje dane osobowe)
- read_repository (umożliwia dostęp do repozytorium tylko do odczytu)

Następnie ustaw ID aplikacji jako zmienną środowiskową:

```bash
export GITLAB_OAUTH_CLIENT_ID=your_application_id_here
```

Więcej informacji znajdziesz na stronie [opencode-gitlab-auth](https://www.npmjs.com/package/@gitlab/opencode-gitlab-auth).

##### Konfiguracja

Customize through `opencode.json`:

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "gitlab": {
      "options": {
        "instanceUrl": "https://gitlab.com",
        "featureFlags": {
          "duo_agent_platform_agentic_chat": true,
          "duo_agent_platform": true
        }
      }
    }
  }
}
```

##### Narzędzia GitLab API (opcjonalne, ale wysoce zalecane)

To access GitLab tools (merge requests, issues, pipelines, CI/CD, etc.):

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "plugin": ["@gitlab/opencode-gitlab-plugin"]
}
```

Ta wtyczka zapewnia kompleksowe możliwości zarządzania repozytorium GitLab, w tym recenzje MR, śledzenie problemów, monitorowanie rurociągów i inne.

---

### GitHub Copilot

Aby korzystać z subskrypcji GitHub Copilot z kodem opencode:

:::note
Niektóre modele mogą wymagać [subskrypcji Pro+](https://github.com/features/Copilot/plans), aby z nich korzystać.

Niektóre modele należy włączyć ręcznie w [ustawieniach GitHub Copilot](https://docs.github.com/en/Copilot/how-tos/use-ai-models/configure-access-to-ai-models#setup-for-individual-use).
:::

1. Uruchom komendę `/connect` i wyszukaj GitHub Copilot.

   ```txt
   /connect
   ```

2. Przejdź do [github.com/login/device](https://github.com/login/device) i wpisz kod.

   ```txt
   ┌ Login with GitHub Copilot
   │
   │ https://github.com/login/device
   │
   │ Enter code: 8F43-6FCF
   │
   └ Waiting for authorization...
   ```

3. Teraz uruchom polecenie `/models`, aby wybrać żądany model.

   ```txt
   /models
   ```

---

### Google Vertex AI

Aby używać Google Vertex AI z opencode:

1. Przejdź do **Model Garden** w Google Cloud Console i sprawdź
   modele dostępne w Twoim regionie.

   :::note
   Musisz mieć projekt Google Cloud z włączonym interfejsem API Vertex AI.
   :::

2. Ustaw wymagane zmienne środowiskowe:
   - `GOOGLE_CLOUD_PROJECT`: identyfikator Twojego projektu Google Cloud
   - `VERTEX_LOCATION` (opcjonalnie): region Vertex AI (domyślnie `global`)
   - Authentication (choose one):
     - `GOOGLE_APPLICATION_CREDENTIALS`: Ścieżka do pliku klucza JSON konta usługi
     - Uwierzytelnij się za pomocą interfejsu CLI gcloud: `gcloud auth application-default login`

   Set them while running opencode.

   ```bash
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json GOOGLE_CLOUD_PROJECT=your-project-id opencode
   ```

   Lub dodaj je do swojego profilu bash.

   ```bash title="~/.bash_profile"
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
   export GOOGLE_CLOUD_PROJECT=your-project-id
   export VERTEX_LOCATION=global
   ```

:::tip
Region `global` poprawia dostępność i zmniejsza liczbę błędów bez dodatkowych kosztów. Użyj regionalnych punktów końcowych (np. `us-central1`) w celu spełnienia wymagań dotyczących miejsca przechowywania danych. [Dowiedz się więcej](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#regional_and_global_endpoints)
:::

3. Uruchom komendę `/models`, aby wybrać żądany model.

   ```txt
   /models
   ```

---

### Groq

1. Przejdź do [konsoli Groq](https://console.groq.com/), kliknij **Utwórz klucz API** i skopiuj klucz.

2. Uruchom komendę `/connect` i wyszukaj Groq.

   ```txt
   /connect
   ```

3. Wprowadź klucz API dla dostawcy.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać żądany.

   ```txt
   /models
   ```

---

### Hugging Face

[Inference Providers Hugging Face](https://huggingface.co/docs/inference-providers) zapewniają dostęp do otwartych modeli obsługiwanych przez ponad 17 dostawców.

1. Przejdź do [ustawień Hugging Face](https://huggingface.co/settings/tokens/new?ownUserPermissions=inference.serverless.write&tokenType=fineGrained), aby utworzyć token z uprawnieniami do wykonywania połączeń z dostawcami wnioskowania.

2. Uruchom komendę `/connect` i wyszukaj **Hugging Face**.

   ```txt
   /connect
   ```

3. Wprowadź swój token Hugging Face.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Kimi-K2-Instruct_ lub _GLM-4.6_.

   ```txt
   /models
   ```

---

### Helicone

[Helicone](https://helicone.ai) to platforma obserwowalności LLM, która zapewnia rejestrowanie, monitorowanie i analizy dla aplikacji AI. Helicone AI Gateway automatycznie kieruje Twoje żądania do odpowiedniego dostawcy w oparciu o model.

1. Udaj się do [Helicone](https://helicone.ai), utwórz konto i wygeneruj klucz API ze swojego panelu.

2. Uruchom komendę `/connect` i wyszukaj **Helicone**.

   ```txt
   /connect
   ```

3. Wprowadź swój klucz API Helicone.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model.

   ```txt
   /models
   ```

Więcej dostawców i zaawansowanych funkcji, takich jak buforowanie i ograniczanie szybkości, znajdziesz w [dokumentacji Helicone](https://docs.helicone.ai).

#### Opcjonalna konfiguracja

Jeśli zobaczysz funkcję lub model firmy Helicone, która nie jest konfigurowana automatycznie za pomocą opencode, zawsze możesz ją skonfigurować samodzielnie.

Oto [Katalog modeli Helicone](https://helicone.ai/models), będziesz go potrzebować, aby pobrać identyfikatory modeli, które chcesz dodać.

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
      },
      "models": {
        "gpt-4o": {
          // Model ID (from Helicone's model directory page)
          "name": "GPT-4o", // Your own custom name for the model
        },
        "claude-sonnet-4-20250514": {
          "name": "Claude Sonnet 4",
        },
      },
    },
  },
}
```

#### Niestandardowe nagłówki

Helicone obsługuje niestandardowe nagłówki dla funkcji takich jak buforowanie, śledzenie użytkowników i zarządzanie sesjami. Dodaj je do konfiguracji dostawcy za pomocą `options.headers`:

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
        "headers": {
          "Helicone-Cache-Enabled": "true",
          "Helicone-User-Id": "opencode",
        },
      },
    },
  },
}
```

##### Śledzenie sesji

Funkcja [Sesje](https://docs.helicone.ai/features/sessions) firmy Helicone umożliwia grupowanie powiązanych żądań LLM. Użyj wtyczki [opencode-helicone-session](https://github.com/H2Shami/opencode-helicone-session), aby automatycznie rejestrować każdą konwersację opencode jako sesję w Helicone.

```bash
npm install -g opencode-helicone-session
```

Dodaj go do swojej konfiguracji.

```json title="opencode.json"
{
  "plugin": ["opencode-helicone-session"]
}
```

Wtyczka wstawia nagłówki `Helicone-Session-Id` i `Helicone-Session-Name` do Twoich żądań. Na stronie Sesje Helicone każda konwersacja opencode będzie wymieniona jako osobna sesja.

##### Najczęstsze nagłówki Helicone

| Header                     | Description                                                   |
| -------------------------- | ------------------------------------------------------------- |
| `Helicone-Cache-Enabled`   | Enable response caching (`true`/`false`)                      |
| `Helicone-User-Id`         | Track metrics by user                                         |
| `Helicone-Property-[Name]` | Add custom properties (e.g., `Helicone-Property-Environment`) |
| `Helicone-Prompt-Id`       | Powiąż żądania z wersjami podpowiedzi                         |

Zobacz [Katalog nagłówków Helicone](https://docs.helicone.ai/helicone-headers/header-directory), aby poznać wszystkie dostępne nagłówki.

---

### llama.cpp

Możesz skonfigurować opencode tak, aby korzystał z modeli lokalnych, za pomocą narzędzia llama-server [llama.cpp](https://github.com/ggml-org/llama.cpp)

```json title="opencode.json" "llama.cpp" {5, 6, 8, 10-15}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "llama.cpp": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "llama-server (local)",
      "options": {
        "baseURL": "http://127.0.0.1:8080/v1"
      },
      "models": {
        "qwen3-coder:a3b": {
          "name": "Qwen3-Coder: a3b-30b (local)",
          "limit": {
            "context": 128000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

W tym przykładzie:

- `llama.cpp` to niestandardowy identyfikator dostawcy. Może to być dowolny ciąg znaków.
- `npm` określa pakiet, który ma być używany dla tego dostawcy. Tutaj `@ai-sdk/openai-compatible` jest używany dla dowolnego API zgodnego z OpenAI.
- `name` to nazwa wyświetlana dostawcy w interfejsie użytkownika.
- `options.baseURL` jest punktem końcowym serwera lokalnego.
- `models` to mapa identyfikatorów modeli do ich konfiguracji. Nazwa modelu zostanie wyświetlona na liście wyboru modelu.

---

### IO.NET

IO.NET oferuje 17 modeli zoptymalizowanych pod kątem różnych zastosowań:

1. Przejdź do [konsoli IO.NET](https://ai.io.net/), utwórz konto i wygeneruj klucz API.

2. Uruchom komendę `/connect` i wyszukaj **IO.NET**.

   ```txt
   /connect
   ```

3. Wprowadź swój klucz API IO.NET.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model.

   ```txt
   /models
   ```

---

### LM Studio

Możesz skonfigurować opencode tak, aby korzystał z modeli lokalnych poprzez LM Studio.

```json title="opencode.json" "lmstudio" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "lmstudio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (local)",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1"
      },
      "models": {
        "google/gemma-3n-e4b": {
          "name": "Gemma 3n-e4b (local)"
        }
      }
    }
  }
}
```

W tym przykładzie:

- `lmstudio` to niestandardowy identyfikator dostawcy. Może to być dowolny ciąg znaków.
- `npm` określa pakiet, który ma być używany dla tego dostawcy. Tutaj `@ai-sdk/openai-compatible` jest używany dla dowolnego API zgodnego z OpenAI.
- `name` to nazwa wyświetlana dostawcy w interfejsie użytkownika.
- `options.baseURL` jest punktem końcowym serwera lokalnego.
- `models` to mapa identyfikatorów modeli do ich konfiguracji. Nazwa modelu zostanie wyświetlona na liście wyboru modelu.

---

### Moonshot AI

Aby użyć Kimi K2 z Moonshot AI:

1. Przejdź do [konsoli Moonshot AI](https://platform.moonshot.ai/console), utwórz konto i kliknij **Utwórz klucz API**.

2. Uruchom polecenie `/connect` i wyszukaj **Moonshot AI**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API Moonshot.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać _Kimi K2_.

   ```txt
   /models
   ```

---

### MiniMax

1. Przejdź do [konsoli API MiniMax](https://platform.minimax.io/login), utwórz konto i wygeneruj klucz API.

2. Uruchom komendę `/connect` i wyszukaj **MiniMax**.

   ```txt
   /connect
   ```

3. Wprowadź swój klucz API MiniMax.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model taki jak _M2.1_.

   ```txt
   /models
   ```

---

### Nebius Token Factory

1. Przejdź do [konsoli Nebius Token Factory](https://tokenfactory.nebius.com/), utwórz konto i kliknij **Dodaj klucz**.

2. Uruchom komendę `/connect` i wyszukaj **Nebius Token Factory**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API Nebius Token Factory.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Kimi K2 Instruct_.

   ```txt
   /models
   ```

---

### Ollama

Możesz skonfigurować opencode tak, aby korzystał z modeli lokalnych poprzez Ollamę.

:::tip
Ollama może automatycznie skonfigurować się pod kątem opencode. Aby uzyskać szczegółowe informacje, zobacz [dokumentację integracji Ollama](https://docs.ollama.com/integrations/opencode).
:::

```json title="opencode.json" "ollama" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (local)",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "llama2": {
          "name": "Llama 2"
        }
      }
    }
  }
}
```

W tym przykładzie:

- `ollama` to niestandardowy identyfikator dostawcy. Może to być dowolny ciąg znaków.
- `npm` określa pakiet, który ma być używany dla tego dostawcy. Tutaj `@ai-sdk/openai-compatible` jest używany dla dowolnego API zgodnego z OpenAI.
- `name` to nazwa wyświetlana dostawcy w interfejsie użytkownika.
- `options.baseURL` jest punktem końcowym serwera lokalnego.
- `models` to mapa identyfikatorów modeli do ich konfiguracji. Nazwa modelu zostanie wyświetlona na liście wyboru modelu.

:::tip
Jeśli wywołania narzędzi nie działają, spróbuj zwiększyć `num_ctx` w Ollama. Zacznij około 16 tys. - 32 tys.
:::

---

### Ollama Cloud

Aby korzystać z Ollama Cloud z opencode:

1. Udaj się do [https://ollama.com/](https://ollama.com/) i zaloguj się lub utwórz konto.

2. Przejdź do **Ustawienia** > **Klucze** i kliknij **Dodaj klucz API**, aby wygenerować nowy klucz API.

3. Skopiuj klucz API do użycia w opencode.

4. Uruchom polecenie `/connect` i wyszukaj **Ollama Cloud**.

   ```txt
   /connect
   ```

5. Wprowadź swój klucz API Ollama Cloud.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

6. **Ważne**: Przed użyciem modeli chmurowych w opencode musisz pobrać informacje o modelu lokalnie:

   ```bash
   ollama pull gpt-oss:20b-cloud
   ```

7. Uruchom polecenie `/models`, aby wybrać model Ollama Cloud.

   ```txt
   /models
   ```

---

### OpenAI

Zalecamy zarejestrowanie się w [ChatGPT Plus lub Pro](https://chatgpt.com/pricing).

1. Po zarejestrowaniu się uruchom komendę `/connect` i wybierz OpenAI.

   ```txt
   /connect
   ```

2. Tutaj możesz wybrać opcję **ChatGPT Plus/Pro**, co spowoduje otwarcie przeglądarki
   i poproś o uwierzytelnienie.

   ```txt
   ┌ Select auth method
   │
   │ ChatGPT Plus/Pro
   │ Manually enter API Key
   └
   ```

3. Teraz wszystkie modele OpenAI powinny być dostępne po użyciu polecenia `/models`.

   ```txt
   /models
   ```

##### Użycie kluczy API

Jeśli posiadasz już klucz API, możesz wybrać opcję **Wprowadź klucz API ręcznie** i wkleić go w terminalu.

---

### OpenCode Zen

OpenCode Zen to lista przetestowanych i zweryfikowanych modeli udostępniona przez zespół opencode. [Dowiedz się więcej](/docs/zen).

1. Zaloguj się do **<a href={console}>OpenCode Zen</a>** i kliknij **Utwórz klucz API**.

2. Uruchom polecenie `/connect` i wyszukaj **OpenCode Zen**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API opencode.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Qwen 3 Coder 480B_.

   ```txt
   /models
   ```

---

### OpenRouter

1. Przejdź do [panelu OpenRouter](https://openrouter.ai/settings/keys), kliknij **Utwórz klucz API** i skopiuj klucz.

2. Uruchom komendę `/connect` i wyszukaj OpenRouter.

   ```txt
   /connect
   ```

3. Wprowadź klucz API dla dostawcy.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Wiele modeli OpenRouter jest domyślnie załadowanych fabrycznie. Uruchom komendę `/models`, aby wybrać ten, który chcesz.

   ```txt
   /models
   ```

   Możesz także dodać dodatkowe modele za pomocą konfiguracji opencode.

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

5. Możesz je także dostosować za pomocą konfiguracji opencode. Oto przykład określenia dostawcy

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "moonshotai/kimi-k2": {
             "options": {
               "provider": {
                 "order": ["baseten"],
                 "allow_fallbacks": false
               }
             }
           }
         }
       }
     }
   }
   ```

---

### SAP AI Core

SAP AI Core zapewnia dostęp do ponad 40 modeli z OpenAI, Anthropic, Google, Amazon, Meta, Mistral i AI21 za pośrednictwem ujednoliconej platformy.

1. Przejdź do [SAP BTP Cockpit](https://account.hana.ondemand.com/), przejdź do instancji usługi SAP AI Core i utwórz klucz usługi.

   :::tip
   Klucz usługi to obiekt JSON zawierający `clientid`, `clientsecret`, `url` i `serviceurls.AI_API_URL`. Instancję AI Core znajdziesz w sekcji **Usługi** > **Instancje i subskrypcje** w Kokpicie BTP.
   :::

2. Uruchom polecenie `/connect` i wyszukaj **SAP AI Core**.

   ```txt
   /connect
   ```

3. Wpisz swój klucz usługi JSON.

   ```txt
   ┌ Service key
   │
   │
   └ enter
   ```

   Lub ustaw zmienną środowiskową `AICORE_SERVICE_KEY`:

   ```bash
   AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}' opencode
   ```

   Lub dodaj go do swojego profilu bash:

   ```bash title="~/.bash_profile"
   export AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}'
   ```

4. Opcjonalnie ustaw identyfikator wdrożenia i grupę zasobów:

   ```bash
   AICORE_DEPLOYMENT_ID=your-deployment-id AICORE_RESOURCE_GROUP=your-resource-group opencode
   ```

   :::note
   Te ustawienia są opcjonalne i należy je skonfigurować zgodnie z konfiguracją SAP AI Core.
   :::

5. Uruchom polecenie `/models`, aby wybrać spośród ponad 40 dostępnych modeli.

   ```txt
   /models
   ```

---

### OVHcloud AI Endpoints

1. Przejdź do [panelu OVHcloud](https://ovh.com/manager). Przejdź do sekcji `Public Cloud`, `AI & Machine Learning` > `AI Endpoints` i na karcie `API Keys` kliknij **Utwórz nowy klucz API**.

2. Uruchom polecenie `/connect` i wyszukaj **Punkty końcowe AI OVHcloud**.

   ```txt
   /connect
   ```

3. Wpisz klucz API punktów końcowych AI OVHcloud.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _gpt-oss-120b_.

   ```txt
   /models
   ```

---

### Scaleway

Aby używać [generatywnych interfejsów API Scaleway](https://www.scaleway.com/en/docs/generative-apis/) z opencode:

1. Przejdź do [Ustawień uprawnień konsoli Scaleway](https://console.scaleway.com/iam/api-keys), aby wygenerować nowy klucz API.

2. Uruchom komendę `/connect` i wyszukaj **Scaleway**.

   ```txt
   /connect
   ```

3. Wprowadź swój klucz API Scaleway.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model taki jak _devstral-2-123b-instruct-2512_ lub _gpt-oss-120b_.

   ```txt
   /models
   ```

---

### Together AI

1. Przejdź do [konsoli Together AI](https://api.together.ai), utwórz konto i kliknij **Dodaj klucz**.

2. Uruchom komendę `/connect` i wyszukaj **Together AI**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API Together AI.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Kimi K2 Instruct_.

   ```txt
   /models
   ```

---

### Venice AI

1. Przejdź do [konsoli Venice AI](https://venice.ai), utwórz konto i wygeneruj klucz API.

2. Uruchom polecenie `/connect` i wyszukaj **Venice AI**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API Venice AI.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Llama 3.3 70B_.

   ```txt
   /models
   ```

---

### Vercel AI Gateway

Vercel AI Gateway umożliwia dostęp do modeli z OpenAI, Anthropic, Google, xAI i innych za pośrednictwem ujednoliconego punktu końcowego. Modele oferowane są po cenie katalogowej bez marży.

1. Przejdź do [panelu Vercel](https://vercel.com/), przejdź do karty **AI Gateway** i kliknij **Klucze API**, aby utworzyć nowy klucz API.

2. Uruchom polecenie `/connect` i wyszukaj **Vercel AI Gateway**.

   ```txt
   /connect
   ```

3. Wprowadź klucz API Vercel AI Gateway.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model.

   ```txt
   /models
   ```

Możesz także dostosować modele za pomocą konfiguracji opencode. Oto przykład określenia kolejności routingu dostawcy.

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "vercel": {
      "models": {
        "anthropic/claude-sonnet-4": {
          "options": {
            "order": ["anthropic", "vertex"]
          }
        }
      }
    }
  }
}
```

Przydatne opcje routingu:

| Opcja               | Opis                                                                         |
| ------------------- | ---------------------------------------------------------------------------- |
| `order`             | Sekwencja dostawcy do wypróbowania                                           |
| `only`              | Ograniczenie do wskazanych dostawców                                         |
| `zeroDataRetention` | Korzystaj wyłącznie z dostawców, którzy nie mają zasad przechowywania danych |

---

### xAI

1. Przejdź do [konsoli xAI](https://console.x.ai/), utwórz konto i wygeneruj klucz API.

2. Uruchom komendę `/connect` i wyszukaj **xAI**.

   ```txt
   /connect
   ```

3. Wprowadź swój klucz API xAI.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom polecenie `/models`, aby wybrać model taki jak _Grok Beta_.

   ```txt
   /models
   ```

---

### Z.AI

1. Przejdź do [konsoli API Z.AI](https://z.ai/manage-apikey/apikey-list), utwórz konto i kliknij **Utwórz nowy klucz API**.

2. Uruchom komendę `/connect` i wyszukaj **Z.AI**.

   ```txt
   /connect
   ```

   Jeżeli jesteś abonentem **Planu kodowania GLM**, wybierz **Plan kodowania Z.AI**.

3. Wpisz swój klucz API Z.AI.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Uruchom komendę `/models`, aby wybrać model taki jak _GLM-4.7_.

   ```txt
   /models
   ```

---

### ZenMux

1. Przejdź do [panelu ZenMux](https://zenmux.ai/settings/keys), kliknij **Utwórz klucz API** i skopiuj klucz.

2. Uruchom polecenie `/connect` i wyszukaj ZenMux.

   ```txt
   /connect
   ```

3. Wprowadź klucz API dla dostawcy.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. Wiele modeli ZenMux jest domyślnie załadowanych fabrycznie. Uruchom komendę `/models`, aby wybrać żądany model.

   ```txt
   /models
   ```

   Możesz także dodać dodatkowe modele za pomocą konfiguracji opencode.

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "zenmux": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

---

## Niestandardowy dostawca

Aby dodać dowolnego dostawcę **kompatybilnego z OpenAI**, którego nie ma na liście w poleceniu `/connect`:

:::tip
Możesz użyć dowolnego dostawcy kompatybilnego z OpenAI z opencode. Większość nowoczesnych dostawców sztucznej inteligencji oferuje interfejsy API kompatybilne z OpenAI.
:::

1. Uruchom polecenie `/connect` i przewiń w dół do **Inne**.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

2. Wprowadź unikalny identyfikator dostawcy.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◇  Enter provider id
   │  myprovider
   └
   ```

   :::note
   Wybierz niezapomniany identyfikator, użyjesz go w swoim pliku konfiguracyjnym.
   :::

3. Wprowadź klucz API dla dostawcy.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ▲  This only stores a credential for myprovider - you will need to configure it in opencode.json, check the docs for examples.
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

4. Utwórz lub zaktualizuj plik `opencode.json` w katalogu projektu:

   ```json title="opencode.json" ""myprovider"" {5-15}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "myprovider": {
         "npm": "@ai-sdk/openai-compatible",
         "name": "My AI ProviderDisplay Name",
         "options": {
           "baseURL": "https://api.myprovider.com/v1"
         },
         "models": {
           "my-model-name": {
             "name": "My Model Display Name"
           }
         }
       }
     }
   }
   ```

   Oto opcje konfiguracji:
   - **npm**: pakiet AI SDK do użycia, `@ai-sdk/openai-compatible` dla dostawców kompatybilnych z OpenAI
   - **name**: Nazwa wyświetlana w UI.
   - **modele**: Dostępne modele.
   - **options.baseURL**: URL endpointu API.
   - **options.apiKey**: Opcjonalnie ustaw klucz API, jeśli nie używasz autoryzacji.
   - **options.headers**: Opcjonalnie ustaw niestandardowe nagłówki.

   Więcej o opcjach zaawansowanych w przykładzie poniżej.

5. Uruchom polecenie `/models`, a niestandardowy dostawca i modele pojawią się na liście wyboru.

---

##### Przykład

Oto przykład ustawienia opcji `apiKey`, `headers` i modelu `limit`.

```json title="opencode.json" {9,11,17-20}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "myprovider": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "My AI ProviderDisplay Name",
      "options": {
        "baseURL": "https://api.myprovider.com/v1",
        "apiKey": "{env:ANTHROPIC_API_KEY}",
        "headers": {
          "Authorization": "Bearer custom-token"
        }
      },
      "models": {
        "my-model-name": {
          "name": "My Model Display Name",
          "limit": {
            "context": 200000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

Szczegóły konfiguracji:

- **apiKey**: Ustaw przy użyciu składni zmiennej `env`, [dowiedz się więcej](/docs/config#env-vars).
- **nagłówki**: niestandardowe nagłówki wysyłane z każdym żądaniem.
- **limit.context**: Maksymalna liczba tokenów wejściowych akceptowanych przez model.
- **limit.output**: Maksymalna liczba tokenów, które model może wygenerować.

Pola `limit` pozwalają opencode zrozumieć, ile kontekstu pozostało. Standardowi dostawcy pobierają je automatycznie z models.dev.

---

## Rozwiązywanie problemów

Jeśli masz problemy z konfiguracją dostawcy, sprawdź następujące elementy:

1. **Sprawdź konfigurację uwierzytelniania**: Uruchom `opencode auth list`, aby sprawdzić, czy poświadczenia
   dla dostawcy są dodawane do Twojej konfiguracji.

   Nie dotyczy to dostawców takich jak Amazon Bedrock, którzy w procesie uwierzytelniania opierają się na zmiennych środowiskowych.

2. W przypadku dostawców niestandardowych sprawdź konfigurację opencode i:
   - Upewnij się, że identyfikator dostawcy użyty w poleceniu `/connect` jest zgodny z identyfikatorem w konfiguracji opencode.
   - Dla dostawcy używany jest właściwy pakiet npm. Na przykład użyj `@ai-sdk/cerebras` dla Cerebras. W przypadku wszystkich innych dostawców zgodnych z OpenAI użyj `@ai-sdk/openai-compatible`.
   - Sprawdź, czy w polu `options.baseURL` użyto prawidłowego punktu końcowego API.
