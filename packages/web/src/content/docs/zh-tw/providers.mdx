---
title: 供應商
description: 使用 OpenCode 中的任何 LLM 供應商。
---

import config from "../../../../config.mjs"
export const console = config.console

OpenCode 使用 [AI SDK](https://ai-sdk.dev/) 和 [Models.dev](https://models.dev) 來支援 **75+ LLM 供應商**，並且它支援執行本地模型。

要添加供應商，您需要：

1. 使用 `/connect` 指令添加供應商的 API 金鑰。
2. 在 OpenCode 配置中配置供應商。

---

### 憑證

當您使用 `/connect` 指令添加供應商的 API 金鑰時，它們會被儲存
在 `~/.local/share/opencode/auth.json`。

---

### 配置

您可以在 OpenCode 配置的 `provider` 部分自定義供應商。

---

#### 基本網址 (Base URL)

您可以透過設定 `baseURL` 選項來自定義任何供應商的基本 URL。這在使用代理服務或自定義端點時非常有用。

```json title="opencode.json" {6}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "anthropic": {
      "options": {
        "baseURL": "https://api.anthropic.com/v1"
      }
    }
  }
}
```

---

## OpenCode Zen

OpenCode Zen 是 OpenCode 團隊提供的模型列表，這些模型已被
經過測試和驗證，可以與 OpenCode 良好配合。 [了解更多](/docs/zen)。

:::tip
如果您是新手，我們建議您從 OpenCode Zen 開始。
:::

1. 在 TUI 中執行 `/connect` 指令，選擇 opencode，然後前往 [opencode.ai/auth](https://opencode.ai/auth)。

   ```txt
   /connect
   ```

2. 登入，添加您的帳單詳細資訊，然後複製您的 API 金鑰。

3. 貼上您的 API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 在 TUI 中執行 `/models` 以查看我們推薦的模型列表。

   ```txt
   /models
   ```

它的工作方式與 OpenCode 中的任何其他供應商一樣，並且完全可以選擇使用。

---

## 目錄

讓我們詳細了解一些供應商。如果您想將供應商添加到
列表，請隨時打開 PR。

:::note
在這裡沒有看到供應商？提交 PR。
:::

---

### 302.AI

1. 前往 [302.AI 主控台](https://302.ai/)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **302.AI**。

   ```txt
   /connect
   ```

3. 輸入您的 302.AI API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型。

   ```txt
   /models
   ```

---

### Amazon Bedrock

要將 Amazon Bedrock 與 OpenCode 結合使用：

1. 前往 Amazon Bedrock 主控台中的 **Model catalog** 並請求
   存取您想要的模型。

   :::tip
   您需要能夠在 Amazon Bedrock 中存取所需的模型。
   :::

2. **使用以下方法之一配置身分驗證**：

   #### 環境變數（快速啟動）

   執行 opencode 時設定以下環境變數之一：

   ```bash
   # Option 1: Using AWS access keys
   AWS_ACCESS_KEY_ID=XXX AWS_SECRET_ACCESS_KEY=YYY opencode

   # Option 2: Using named AWS profile
   AWS_PROFILE=my-profile opencode

   # Option 3: Using Bedrock bearer token
   AWS_BEARER_TOKEN_BEDROCK=XXX opencode
   ```

   或者將它們添加到您的 bash 設定檔中：

   ```bash title="~/.bash_profile"
   export AWS_PROFILE=my-dev-profile
   export AWS_REGION=us-east-1
   ```

   #### 設定檔（推薦）

   對於特定於專案或持久的配置，請使用 `opencode.json`：

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "my-aws-profile"
         }
       }
     }
   }
   ```

   **可用選項：**
   - `region` - AWS 區域（例如 `us-east-1`、`eu-west-1`）
   - `profile` - 來自 `~/.aws/credentials` 的 AWS 命名設定檔
   - `endpoint` - 用於 VPC 端點的自定義端點 URL（通用 `baseURL` 選項的別名）

   :::tip
   設定檔選項優先於環境變數。
   :::

   #### 高級：VPC 端點

   如果您使用 Bedrock 的 VPC 端點：

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "production",
           "endpoint": "https://bedrock-runtime.us-east-1.vpce-xxxxx.amazonaws.com"
         }
       }
     }
   }
   ```

   :::note
   `endpoint` 選項是通用 `baseURL` 選項的別名，使用 AWS 特定術語。如果同時指定 `endpoint` 和 `baseURL`，則 `endpoint` 優先。
   :::

   #### 認證方式
   - **`AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`**：建立 IAM 使用者並在 AWS Console 中生成存取金鑰
   - **`AWS_PROFILE`**：使用 `~/.aws/credentials` 中的命名設定檔。首先配置 `aws configure --profile my-profile` 或 `aws sso login`
   - **`AWS_BEARER_TOKEN_BEDROCK`**：從 Amazon Bedrock 主控台生成長期 API 金鑰
   - **`AWS_WEB_IDENTITY_TOKEN_FILE` / `AWS_ROLE_ARN`**：適用於 EKS IRSA（服務帳戶的 IAM 角色）或具有 OIDC 聯合的其他 Kubernetes 環境。使用服務帳戶註解時，這些環境變數由 Kubernetes 自動注入。

   #### 認證優先級

   Amazon Bedrock 使用以下身分驗證優先級：
   1. **Bearer Token** - `AWS_BEARER_TOKEN_BEDROCK` 環境變數或來自 `/connect` 指令的令牌
   2. **AWS Credential Chain** - 設定檔、存取金鑰、共享憑證、IAM 角色、Web 身分令牌 (EKS IRSA)、實例元數據

   :::note
   當設定 Bearer Token 時（透過 `/connect` 或 `AWS_BEARER_TOKEN_BEDROCK`），它優先於所有 AWS 憑證方法（包括配置的設定檔）。
   :::

3. 執行 `/models` 指令選擇所需的模型。

   ```txt
   /models
   ```

:::note
對於自定義推理設定檔，請在鍵中使用模型和供應商名稱，並將 `id` 屬性設定為 arn。這確保了正確的快取：

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "amazon-bedrock": {
      // ...
      "models": {
        "anthropic-claude-sonnet-4.5": {
          "id": "arn:aws:bedrock:us-east-1:xxx:application-inference-profile/yyy"
        }
      }
    }
  }
}
```

:::

---

### Anthropic

1. 註冊後，執行 `/connect` 指令並選擇 Anthropic。

   ```txt
   /connect
   ```

2. 在這裡您可以選擇 **Claude Pro/Max** 選項，它將打開您的瀏覽器
   並要求您進行身分驗證。

   ```txt
   ┌ Select auth method
   │
   │ Claude Pro/Max
   │ Create an API Key
   │ Manually enter API Key
   └
   ```

3. 現在，當您使用 `/models` 指令時，所有 Anthropic 模型都應該可用。

   ```txt
   /models
   ```

:::info
[Anthropic](https://anthropic.com) 不正式支援在 OpenCode 中使用您的 Claude Pro/Max 訂閱。
:::

##### 使用 API 金鑰

如果您沒有 Pro/Max 訂閱，您還可以選擇 **Create an API Key**。它還會打開您的瀏覽器並要求您登入 Anthropic 並為您提供一個可以貼上到終端機中的代碼。

或者，如果您已有 API 金鑰，則可以選擇 **Manually enter API Key** 並將其貼上到您的終端機中。

---

### Azure OpenAI

:::note
如果遇到 "I'm sorry, but I cannot assist with that request" 錯誤，請嘗試將 Azure 資源中的內容篩選器從 **DefaultV2** 更改為 **Default**。
:::

1. 轉到 [Azure portal](https://portal.azure.com/) 並建立 **Azure OpenAI** 資源。您需要：
   - **Resource name**：這將成為您的 API 端點 (`https://RESOURCE_NAME.openai.azure.com/`) 的一部分
   - **API key**：來自您的資源的 `KEY 1` 或 `KEY 2`

2. 轉到 [Azure AI Foundry](https://ai.azure.com/) 並部署模型。

   :::note
   部署名稱必須與模型名稱匹配，OpenCode 才能正常工作。
   :::

3. 執行 `/connect` 指令並搜尋 **Azure**。

   ```txt
   /connect
   ```

4. 輸入您的 API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. 將您的資源名稱設定為環境變數：

   ```bash
   AZURE_RESOURCE_NAME=XXX opencode
   ```

   或者將其添加到您的 bash 設定檔中：

   ```bash title="~/.bash_profile"
   export AZURE_RESOURCE_NAME=XXX
   ```

6. 執行 `/models` 指令以選擇您部署的模型。

   ```txt
   /models
   ```

---

### Azure Cognitive Services

1. 轉到 [Azure portal](https://portal.azure.com/) 並建立 **Azure OpenAI** 資源。您需要：
   - **Resource name**：這將成為您的 API 端點 (`https://AZURE_COGNITIVE_SERVICES_RESOURCE_NAME.cognitiveservices.azure.com/`) 的一部分
   - **API key**：來自您的資源的 `KEY 1` 或 `KEY 2`

2. 轉到 [Azure AI Foundry](https://ai.azure.com/) 並部署模型。

   :::note
   部署名稱必須與模型名稱匹配，OpenCode 才能正常工作。
   :::

3. 執行 `/connect` 指令並搜尋 **Azure Cognitive Services**。

   ```txt
   /connect
   ```

4. 輸入您的 API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. 將您的資源名稱設定為環境變數：

   ```bash
   AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX opencode
   ```

   或者將其添加到您的 bash 設定檔中：

   ```bash title="~/.bash_profile"
   export AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX
   ```

6. 執行 `/models` 指令以選擇您部署的模型。

   ```txt
   /models
   ```

---

### Baseten

1. 前往 [Baseten](https://app.baseten.co/)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Baseten**。

   ```txt
   /connect
   ```

3. 輸入您的 Baseten API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型。

   ```txt
   /models
   ```

---

### Cerebras

1. 前往 [Cerebras console](https://inference.cerebras.ai/)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Cerebras**。

   ```txt
   /connect
   ```

3. 輸入您的 Cerebras API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型，如 _Qwen 3 Coder 480B_。

   ```txt
   /models
   ```

---

### Cloudflare AI Gateway

Cloudflare AI Gateway 讓您可以透過統一端點存取來自 OpenAI、Anthropic、Workers AI 等的模型。使用 [Unified Billing](https://developers.cloudflare.com/ai-gateway/features/unified-billing/)，您不需要為每個供應商提供單獨的 API 金鑰。

1. 前往 [Cloudflare dashboard](https://dash.cloudflare.com/)，導覽至 **AI** > **AI Gateway**，然後建立一個新閘道。

2. 將您的 Account ID 和 Gateway ID 設定為環境變數。

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_ACCOUNT_ID=your-32-character-account-id
   export CLOUDFLARE_GATEWAY_ID=your-gateway-id
   ```

3. 執行 `/connect` 指令並搜尋 **Cloudflare AI Gateway**。

   ```txt
   /connect
   ```

4. 輸入您的 Cloudflare API 令牌。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

   或者將其設定為環境變數。

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_API_TOKEN=your-api-token
   ```

5. 執行 `/models` 指令選擇模型。

   ```txt
   /models
   ```

   您還可以透過 OpenCode 配置添加模型。

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "cloudflare-ai-gateway": {
         "models": {
           "openai/gpt-4o": {},
           "anthropic/claude-sonnet-4": {}
         }
       }
     }
   }
   ```

---

### Cortecs

1. 前往 [Cortecs console](https://cortecs.ai/)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Cortecs**。

   ```txt
   /connect
   ```

3. 輸入您的 Cortecs API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇一個模型，如 _Kimi K2 Instruct_。

   ```txt
   /models
   ```

---

### DeepSeek

1. 前往 [DeepSeek console](https://platform.deepseek.com/)，建立一個帳戶，然後單擊 **Create new API key**。

2. 執行 `/connect` 指令並搜尋 **DeepSeek**。

   ```txt
   /connect
   ```

3. 輸入您的 DeepSeek API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令以選擇 DeepSeek 模型，例如 _DeepSeek Reasoner_。

   ```txt
   /models
   ```

---

### Deep Infra

1. 前往 [Deep Infra dashboard](https://deepinfra.com/dash)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Deep Infra**。

   ```txt
   /connect
   ```

3. 輸入您的 Deep Infra API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型。

   ```txt
   /models
   ```

---

### Firmware

1. 前往 [Firmware dashboard](https://app.firmware.ai/signup)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Firmware**。

   ```txt
   /connect
   ```

3. 輸入您的 Firmware API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型。

   ```txt
   /models
   ```

---

### Fireworks AI

1. 前往 [Fireworks AI console](https://app.fireworks.ai/)，建立一個帳戶，然後單擊 **Create API Key**。

2. 執行 `/connect` 指令並搜尋 **Fireworks AI**。

   ```txt
   /connect
   ```

3. 輸入您的 Fireworks AI API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇一個模型，如 _Kimi K2 Instruct_。

   ```txt
   /models
   ```

---

### GitLab Duo

GitLab Duo 透過 GitLab 的 Anthropic 代理提供具有本機工具呼叫功能的 AI 驅動的代理聊天。

1. 執行 `/connect` 指令並選擇 GitLab。

   ```txt
   /connect
   ```

2. 選擇您的身分驗證方法：

   ```txt
   ┌ Select auth method
   │
   │ OAuth (Recommended)
   │ Personal Access Token
   │
   └
   ```

   #### 使用 OAuth（推薦）

   選擇 **OAuth**，您的瀏覽器將打開以進行授權。

   #### 使用 Personal Access Token
   1. 前往 [GitLab User Settings > Access Tokens](https://gitlab.com/-/user_settings/personal_access_tokens)
   2. 單擊 **Add new token**
   3. Name：`OpenCode`，Scopes：`api`
   4. 複製令牌（以 `glpat-` 開頭）
   5. 在終端機中輸入它

3. 執行 `/models` 指令以查看可用模型。

   ```txt
   /models
   ```

   提供三種基於 Claude 的模型：
   - **duo-chat-haiku-4-5**（Default）- 快速任務的快速回應
   - **duo-chat-sonnet-4-5** - 大多數工作流程的平衡性能
   - **duo-chat-opus-4-5** - 最有能力進行複雜分析

:::note
如果您不想將令牌儲存在 OpenCode auth 儲存中，也可以指定 `GITLAB_TOKEN` 環境變數。
:::

##### 自託管 GitLab

:::note[合規性注意事項]
OpenCode 使用小型模型來執行某些 AI 任務，例如生成工作階段標題。
預設情況下，它配置為使用 gpt-5-nano，由 Zen 託管。若要鎖定 OpenCode
僅使用您自己的 GitLab 託管實例，請將以下內容添加到您的
`opencode.json` 檔案。還建議禁用工作階段分享。

```json
{
  "$schema": "https://opencode.ai/config.json",
  "small_model": "gitlab/duo-chat-haiku-4-5",
  "share": "disabled"
}
```

:::

對於自託管的 GitLab 實例：

```bash
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_TOKEN=glpat-...
```

如果您的實例執行自定義 AI 閘道：

```bash
GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
```

或者添加到您的 bash 設定檔中：

```bash title="~/.bash_profile"
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
export GITLAB_TOKEN=glpat-...
```

:::note
您的 GitLab 管理員必須啟用以下功能：

1. [Duo Agent Platform](https://docs.gitlab.com/user/gitlab_duo/turn_on_off/) 用於使用者、組或實例
2. Feature flags（透過 Rails console）：
   - `agent_platform_claude_code`
   - `third_party_agents_enabled`
     :::

##### 自託管實例的 OAuth

為了使 OAuth 適用於您的自託管實例，您需要建立
一個新的應用程式（Settings → Applications），使用
回調 URL `http://127.0.0.1:8080/callback` 和以下範圍：

- api（代表您存取 API）
- read_user（讀取您的個人資訊）
- read_repository（允許對儲存庫進行唯讀存取）

然後將應用程式 ID 公開為環境變數：

```bash
export GITLAB_OAUTH_CLIENT_ID=your_application_id_here
```

更多文件請參見 [opencode-gitlab-auth](https://www.npmjs.com/package/@gitlab/opencode-gitlab-auth) 主頁。

##### 配置

透過 `opencode.json` 定制：

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "gitlab": {
      "options": {
        "instanceUrl": "https://gitlab.com",
        "featureFlags": {
          "duo_agent_platform_agentic_chat": true,
          "duo_agent_platform": true
        }
      }
    }
  }
}
```

##### GitLab API Tools（可選，但強烈推薦）

要存取 GitLab 工具（merge requests、issues、pipelines、CI/CD 等）：

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "plugin": ["@gitlab/opencode-gitlab-plugin"]
}
```

該外掛提供全面的 GitLab 儲存庫管理功能，包括 MR 審閱、問題追蹤、管線監控等。

---

### GitHub Copilot

要將 GitHub Copilot 訂閱與 OpenCode 結合使用：

:::note
某些模型可能需要 [Pro+
subscription](https://github.com/features/copilot/plans) 來使用。
:::

1. 執行 `/connect` 指令並搜尋 GitHub Copilot。

   ```txt
   /connect
   ```

2. 導覽至 [github.com/login/device](https://github.com/login/device) 並輸入代碼。

   ```txt
   ┌ Login with GitHub Copilot
   │
   │ https://github.com/login/device
   │
   │ Enter code: 8F43-6FCF
   │
   └ Waiting for authorization...
   ```

3. 現在執行 `/models` 指令來選擇您想要的模型。

   ```txt
   /models
   ```

---

### Google Vertex AI

要將 Google Vertex AI 與 OpenCode 結合使用：

1. 前往 Google Cloud Console 中的 **Model Garden** 並檢查
   您所在地區提供的模型。

   :::note
   您需要有一個啟用了 Vertex AI API 的 Google Cloud 專案。
   :::

2. 設定所需的環境變數：
   - `GOOGLE_CLOUD_PROJECT`：您的 Google Cloud 專案 ID
   - `VERTEX_LOCATION`（可選）：Vertex AI 的區域（預設為 `global`）
   - 身分驗證（選擇一項）：
     - `GOOGLE_APPLICATION_CREDENTIALS`：Path to your service account JSON key file
     - Authenticate using gcloud CLI: `gcloud auth application-default login`

   在執行 opencode 時設定它們。

   ```bash
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json GOOGLE_CLOUD_PROJECT=your-project-id opencode
   ```

   或者將它們添加到您的 bash 設定檔中。

   ```bash title="~/.bash_profile"
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
   export GOOGLE_CLOUD_PROJECT=your-project-id
   export VERTEX_LOCATION=global
   ```

:::tip
`global` 區域無需額外成本即可提高可用性並減少錯誤。使用區域端點（例如 `us-central1`）來滿足資料駐留要求。 [了解更多](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#regional_and_global_endpoints)
:::

3. 執行 `/models` 指令選擇所需的模型。

   ```txt
   /models
   ```

---

### Groq

1. 前往 [Groq console](https://console.groq.com/)，單擊 **Create API Key**，然後複製金鑰。

2. 執行 `/connect` 指令並搜尋 Groq。

   ```txt
   /connect
   ```

3. 輸入供應商的 API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇您想要的。

   ```txt
   /models
   ```

---

### Hugging Face

[Hugging Face Inference Providers](https://huggingface.co/docs/inference-providers) 提供對超過 17 個供應商支援的開放模型的存取。

1. 前往 [Hugging Face settings](https://huggingface.co/settings/tokens/new?ownUserPermissions=inference.serverless.write&tokenType=fineGrained) 建立一個具有呼叫 Inference Providers 權限的令牌。

2. 執行 `/connect` 指令並搜尋 **Hugging Face**。

   ```txt
   /connect
   ```

3. 輸入您的 Hugging Face 令牌。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令來選擇模型，如 _Kimi-K2-Instruct_ 或 _GLM-4.6_。

   ```txt
   /models
   ```

---

### Helicone

[Helicone](https://helicone.ai) 是一個 LLM 可觀測性平台，可為您的 AI 應用程式提供記錄、監控和分析。 The Helicone AI Gateway 根據模型自動將您的請求路由到適當的供應商。

1. 前往 [Helicone](https://helicone.ai)，建立一個帳戶，並從您的儀表板生成 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Helicone**。

   ```txt
   /connect
   ```

3. 輸入您的 Helicone API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型。

   ```txt
   /models
   ```

有關更多供應商和高級功能（例如快取和速率限制），請查看 [Helicone documentation](https://docs.helicone.ai)。

#### 可選配置

如果您發現 Helicone 的功能或模型未透過 OpenCode 自動配置，您始終可以自行配置。

這是 [Helicone's Model Directory](https://helicone.ai/models)，您將需要它來獲取要添加的模型的 ID。

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
      },
      "models": {
        "gpt-4o": {
          // Model ID (from Helicone's model directory page)
          "name": "GPT-4o", // Your own custom name for the model
        },
        "claude-sonnet-4-20250514": {
          "name": "Claude Sonnet 4",
        },
      },
    },
  },
}
```

#### 自定義標頭

Helicone 支援快取、使用者追蹤和工作階段管理等功能的自定義標頭。使用 `options.headers` 將它們添加到您的供應商配置中：

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
        "headers": {
          "Helicone-Cache-Enabled": "true",
          "Helicone-User-Id": "opencode",
        },
      },
    },
  },
}
```

##### 工作階段追蹤

Helicone 的 [Sessions](https://docs.helicone.ai/features/sessions) 功能可讓您將相關的 LLM 請求分組在一起。使用 [opencode-helicone-session](https://github.com/H2Shami/opencode-helicone-session) 外掛自動將每個 OpenCode 對話記錄為 Helicone 中的工作階段。

```bash
npm install -g opencode-helicone-session
```

將其添加到您的配置中。

```json title="opencode.json"
{
  "plugin": ["opencode-helicone-session"]
}
```

該外掛將 `Helicone-Session-Id` 和 `Helicone-Session-Name` 標頭注入您的請求中。在 Helicone 的 Sessions 頁面中，您將看到每個 OpenCode 對話都列為單獨的工作階段。

##### 常見 Helicone 標頭

| Header                     | Description                                            |
| -------------------------- | ------------------------------------------------------ |
| `Helicone-Cache-Enabled`   | 啟用回應快取 (`true`/`false`)                          |
| `Helicone-User-Id`         | 按使用者追蹤指標                                       |
| `Helicone-Property-[Name]` | 添加自定義屬性（例如 `Helicone-Property-Environment`） |
| `Helicone-Prompt-Id`       | 將請求與提示版本相關聯                                 |

有關所有可用標頭，請參閱 [Helicone Header Directory](https://docs.helicone.ai/helicone-headers/header-directory)。

---

### llama.cpp

您可以透過 [llama.cpp's](https://github.com/ggml-org/llama.cpp) llama-server 公用程式配置 OpenCode 以使用本地模型

```json title="opencode.json" "llama.cpp" {5, 6, 8, 10-15}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "llama.cpp": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "llama-server (local)",
      "options": {
        "baseURL": "http://127.0.0.1:8080/v1"
      },
      "models": {
        "qwen3-coder:a3b": {
          "name": "Qwen3-Coder: a3b-30b (local)",
          "limit": {
            "context": 128000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

在這個例子中：

- `llama.cpp` 是自定義供應商 ID。這可以是您想要的任何字串。
- `npm` 指定用於此供應商的套件。這裡，`@ai-sdk/openai-compatible` 用於任何 OpenAI 相容的 API。
- `name` 是 UI 中供應商的顯示名稱。
- `options.baseURL` 是本地伺服器的端點。
- `models` 是模型 ID 與其配置的對應。模型名稱將顯示在模型選擇列表中。

---

### IO.NET

IO.NET 提供了 17 種針對各種使用案例進行最佳化的模型：

1. 前往 [IO.NET console](https://ai.io.net/)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **IO.NET**。

   ```txt
   /connect
   ```

3. 輸入您的 IO.NET API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型。

   ```txt
   /models
   ```

---

### LM Studio

您可以透過 LM Studio 配置 OpenCode 以使用本地模型。

```json title="opencode.json" "lmstudio" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "lmstudio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (local)",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1"
      },
      "models": {
        "google/gemma-3n-e4b": {
          "name": "Gemma 3n-e4b (local)"
        }
      }
    }
  }
}
```

在這個例子中：

- `lmstudio` 是自定義供應商 ID。這可以是您想要的任何字串。
- `npm` 指定用於此供應商的套件。這裡，`@ai-sdk/openai-compatible` 用於任何 OpenAI 相容的 API。
- `name` 是 UI 中供應商的顯示名稱。
- `options.baseURL` 是本地伺服器的端點。
- `models` 是模型 ID 與其配置的對應。模型名稱將顯示在模型選擇列表中。

---

### Moonshot AI

要使用 Moonshot AI 中的 Kimi K2：

1. 前往 [Moonshot AI console](https://platform.moonshot.ai/console)，建立一個帳戶，然後單擊 **Create API key**。

2. 執行 `/connect` 指令並搜尋 **Moonshot AI**。

   ```txt
   /connect
   ```

3. 輸入您的 Moonshot API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇 _Kimi K2_。

   ```txt
   /models
   ```

---

### MiniMax

1. 前往 [MiniMax API Console](https://platform.minimax.io/login)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **MiniMax**。

   ```txt
   /connect
   ```

3. 輸入您的 MiniMax API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型，如 _M2.1_。

   ```txt
   /models
   ```

---

### Nebius Token Factory

1. 前往 [Nebius Token Factory console](https://tokenfactory.nebius.com/)，建立一個帳戶，然後單擊 **Add Key**。

2. 執行 `/connect` 指令並搜尋 **Nebius Token Factory**。

   ```txt
   /connect
   ```

3. 輸入您的 Nebius Token Factory API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇一個模型，如 _Kimi K2 Instruct_。

   ```txt
   /models
   ```

---

### Ollama

您可以透過 Ollama 配置 OpenCode 以使用本地模型。

:::tip
Ollama 可以自動為 OpenCode 配置自身。詳情請參閱 [Ollama integration docs](https://docs.ollama.com/integrations/opencode)。
:::

```json title="opencode.json" "ollama" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (local)",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "llama2": {
          "name": "Llama 2"
        }
      }
    }
  }
}
```

在這個例子中：

- `ollama` 是自定義供應商 ID。這可以是您想要的任何字串。
- `npm` 指定用於此供應商的套件。這裡，`@ai-sdk/openai-compatible` 用於任何 OpenAI 相容的 API。
- `name` 是 UI 中供應商的顯示名稱。
- `options.baseURL` 是本地伺服器的端點。
- `models` 是模型 ID 與其配置的對應。模型名稱將顯示在模型選擇列表中。

:::tip
如果工具呼叫不起作用，請嘗試增加 Ollama 中的 `num_ctx`。從 16k - 32k 左右開始。
:::

---

### Ollama Cloud

要將 Ollama Cloud 與 OpenCode 結合使用：

1. 前往 [https://ollama.com/](https://ollama.com/) 並登入或建立帳戶。

2. 導覽到 **Settings** > **Keys**，然後單擊 **Add API Key** 以生成新的 API 金鑰。

3. 複製 API 金鑰以在 OpenCode 中使用。

4. 執行 `/connect` 指令並搜尋 **Ollama Cloud**。

   ```txt
   /connect
   ```

5. 輸入您的 Ollama Cloud API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

6. **Important**：在 OpenCode 中使用雲端模型之前，您必須將模型資訊拉取到本地：

   ```bash
   ollama pull gpt-oss:20b-cloud
   ```

7. 執行 `/models` 指令以選擇您的 Ollama Cloud 模型。

   ```txt
   /models
   ```

---

### OpenAI

我們建議註冊 [ChatGPT Plus or Pro](https://chatgpt.com/pricing)。

1. 註冊後，執行 `/connect` 指令並選擇 OpenAI。

   ```txt
   /connect
   ```

2. 在這裡您可以選擇 **ChatGPT Plus/Pro** 選項，它將打開您的瀏覽器
   並要求您進行身分驗證。

   ```txt
   ┌ Select auth method
   │
   │ ChatGPT Plus/Pro
   │ Manually enter API Key
   └
   ```

3. 現在，當您使用 `/models` 指令時，所有 OpenAI 模型都應該可用。

   ```txt
   /models
   ```

##### 使用 API 金鑰

如果您已有 API 金鑰，可以選擇 **Manually enter API Key** 並將其貼上到您的終端機中。

---

### OpenCode Zen

OpenCode Zen 是 OpenCode 團隊提供的經過測試和驗證的模型列表。 [了解更多](/docs/zen)。

1. 登入 **<a href={console}>OpenCode Zen</a>** 並單擊 **Create API Key**。

2. 執行 `/connect` 指令並搜尋 **OpenCode Zen**。

   ```txt
   /connect
   ```

3. 輸入您的 OpenCode API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型，如 _Qwen 3 Coder 480B_。

   ```txt
   /models
   ```

---

### OpenRouter

1. 前往 [OpenRouter dashboard](https://openrouter.ai/settings/keys)，單擊 **Create API Key**，然後複製金鑰。

2. 執行 `/connect` 指令並搜尋 OpenRouter。

   ```txt
   /connect
   ```

3. 輸入供應商的 API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 許多 OpenRouter 模型是預設預先載入的，執行 `/models` 指令選擇您想要的模型。

   ```txt
   /models
   ```

   您還可以透過 OpenCode 配置添加其他模型。

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

5. 您還可以透過 OpenCode 配置自定義它們。這是指定供應商的範例

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "moonshotai/kimi-k2": {
             "options": {
               "provider": {
                 "order": ["baseten"],
                 "allow_fallbacks": false
               }
             }
           }
         }
       }
     }
   }
   ```

---

### SAP AI Core

SAP AI Core 透過統一平台提供對 OpenAI、Anthropic、Google、Amazon、Meta、Mistral 和 AI21 的 40 多個模型的存取。

1. 轉到您的 [SAP BTP Cockpit](https://account.hana.ondemand.com/)，導覽到您的 SAP AI Core 服務實例，然後建立一個服務金鑰。

   :::tip
   服務金鑰是一個包含 `clientid`、`clientsecret`、`url` 和 `serviceurls.AI_API_URL` 的 JSON 物件。您可以在 BTP Cockpit 的 **Services** > **Instances and Subscriptions** 下找到您的 AI Core 實例。
   :::

2. 執行 `/connect` 指令並搜尋 **SAP AI Core**。

   ```txt
   /connect
   ```

3. 輸入您的服務金鑰 JSON。

   ```txt
   ┌ Service key
   │
   │
   └ enter
   ```

   或者設定 `AICORE_SERVICE_KEY` 環境變數：

   ```bash
   AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}' opencode
   ```

   或者將其添加到您的 bash 設定檔中：

   ```bash title="~/.bash_profile"
   export AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}'
   ```

4. （可選）設定部署 ID 和資源組：

   ```bash
   AICORE_DEPLOYMENT_ID=your-deployment-id AICORE_RESOURCE_GROUP=your-resource-group opencode
   ```

   :::note
   這些設定是可選的，應根據您的 SAP AI Core 設定進行配置。
   :::

5. 執行 `/models` 指令從 40 多個可用模型中進行選擇。

   ```txt
   /models
   ```

---

### OVHcloud AI Endpoints

1. 前往 [OVHcloud panel](https://ovh.com/manager)。導覽到 `Public Cloud` 部分，`AI & Machine Learning` > `AI Endpoints`，然後在 `API Keys` 選項卡中單擊 **Create a new API key**。

2. 執行 `/connect` 指令並搜尋 **OVHcloud AI Endpoints**。

   ```txt
   /connect
   ```

3. 輸入您的 OVHcloud AI Endpoints API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型，如 _gpt-oss-120b_。

   ```txt
   /models
   ```

---

### Scaleway

要將 [Scaleway Generative APIs](https://www.scaleway.com/en/docs/generative-apis/) 與 OpenCode 一起使用：

1. 前往 [Scaleway Console IAM settings](https://console.scaleway.com/iam/api-keys) 生成新的 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Scaleway**。

   ```txt
   /connect
   ```

3. 輸入您的 Scaleway API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型，如 _devstral-2-123b-instruct-2512_ 或 _gpt-oss-120b_。

   ```txt
   /models
   ```

---

### Together AI

1. 前往 [Together AI console](https://api.together.ai)，建立一個帳戶，然後單擊 **Add Key**。

2. 執行 `/connect` 指令並搜尋 **Together AI**。

   ```txt
   /connect
   ```

3. 輸入您的 Together AI API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇一個模型，如 _Kimi K2 Instruct_。

   ```txt
   /models
   ```

---

### Venice AI

1. 前往 [Venice AI console](https://venice.ai)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Venice AI**。

   ```txt
   /connect
   ```

3. 輸入您的 Venice AI API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型，如 _Llama 3.3 70B_。

   ```txt
   /models
   ```

---

### Vercel AI Gateway

Vercel AI Gateway 可讓您透過統一端點存取來自 OpenAI、Anthropic、Google、xAI 等的模型。模型按定價提供，不加價。

1. 前往 [Vercel dashboard](https://vercel.com/)，導覽至 **AI Gateway** 選項卡，然後單擊 **API keys** 以建立新的 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **Vercel AI Gateway**。

   ```txt
   /connect
   ```

3. 輸入您的 Vercel AI Gateway API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇模型。

   ```txt
   /models
   ```

您還可以透過 OpenCode 配置自定義模型。以下是指定供應商路由順序的範例。

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "vercel": {
      "models": {
        "anthropic/claude-sonnet-4": {
          "options": {
            "order": ["anthropic", "vertex"]
          }
        }
      }
    }
  }
}
```

一些有用的路由選項：

| Option              | Description                                          |
| ------------------- | ---------------------------------------------------- |
| `order`             | Provider sequence to try                             |
| `only`              | Restrict to specific providers                       |
| `zeroDataRetention` | Only use providers with zero data retention policies |

---

### xAI

1. 前往 [xAI console](https://console.x.ai/)，建立一個帳戶並生成一個 API 金鑰。

2. 執行 `/connect` 指令並搜尋 **xAI**。

   ```txt
   /connect
   ```

3. 輸入您的 xAI API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇一個模型，如 _Grok Beta_。

   ```txt
   /models
   ```

---

### Z.AI

1. 前往 [Z.AI API console](https://z.ai/manage-apikey/apikey-list)，建立一個帳戶，然後單擊 **Create a new API key**。

2. 執行 `/connect` 指令並搜尋 **Z.AI**。

   ```txt
   /connect
   ```

   如果您訂閱了 **GLM Coding Plan**，請選擇 **Z.AI Coding Plan**。

3. 輸入您的 Z.AI API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 執行 `/models` 指令選擇 _GLM-4.7_ 等模型。

   ```txt
   /models
   ```

---

### ZenMux

1. 前往 [ZenMux dashboard](https://zenmux.ai/settings/keys)，單擊 **Create API Key**，然後複製金鑰。

2. 執行 `/connect` 指令並搜尋 ZenMux。

   ```txt
   /connect
   ```

3. 輸入供應商的 API 金鑰。

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 預設情況下預先載入了許多 ZenMux 模型，執行 `/models` 指令選擇您想要的模型。

   ```txt
   /models
   ```

   您還可以透過 OpenCode 配置添加其他模型。

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "zenmux": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

---

## 自定義供應商

要添加 `/connect` 指令中未列出的任何 **OpenAI-compatible** 供應商：

:::tip
您可以將任何與 OpenAI 相容的供應商與 OpenCode 結合使用。大多數現代 AI 供應商都提供與 OpenAI 相容的 API。
:::

1. 執行 `/connect` 指令並向下捲動到 **Other**。

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

2. 輸入供應商的唯一 ID。

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◇  Enter provider id
   │  myprovider
   └
   ```

   :::note
   選擇一個容易記住的 ID，您將在設定檔中使用它。
   :::

3. 輸入供應商的 API 金鑰。

   ```bash
   $ /connect

   ┌  Add credential
   │
   ▲  This only stores a credential for myprovider - you will need to configure it in opencode.json, check the docs for examples.
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

4. 在專案目錄中建立或更新 `opencode.json` 檔案：

   ```json title="opencode.json" ""myprovider"" {5-15}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "myprovider": {
         "npm": "@ai-sdk/openai-compatible",
         "name": "My AI ProviderDisplay Name",
         "options": {
           "baseURL": "https://api.myprovider.com/v1"
         },
         "models": {
           "my-model-name": {
             "name": "My Model Display Name"
           }
         }
       }
     }
   }
   ```

   以下是配置選項：
   - **npm**：要使用的 AI SDK 套件，`@ai-sdk/openai-compatible` 用於 OpenAI 相容供應商
   - **name**：UI 中的顯示名稱。
   - **models**：可用模型。
   - **options.baseURL**：API 端點 URL。
   - **options.apiKey**：如果不使用身分驗證，可以選擇設定 API 金鑰。
   - **options.headers**：可選擇設定自定義標頭。

   有關高級選項的更多資訊，請參見下面的範例。

5. 執行 `/models` 指令，您的自定義供應商和模型將出現在選擇列表中。

---

##### 範例

以下是設定 `apiKey`、`headers` 和模型 `limit` 選項的範例。

```json title="opencode.json" {9,11,17-20}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "myprovider": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "My AI ProviderDisplay Name",
      "options": {
        "baseURL": "https://api.myprovider.com/v1",
        "apiKey": "{env:ANTHROPIC_API_KEY}",
        "headers": {
          "Authorization": "Bearer custom-token"
        }
      },
      "models": {
        "my-model-name": {
          "name": "My Model Display Name",
          "limit": {
            "context": 200000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

配置詳情：

- **apiKey**：使用 `env` 變數語法 [了解更多](/docs/config#env-vars) 設定。
- **headers**：隨每個請求發送的自定義標頭。
- **limit.context**：模型接受的最大輸入 tokens。
- **limit.output**：模型可以生成的最大 tokens。

`limit` 欄位允許 OpenCode 了解您還剩下多少上下文。標準供應商會自動從 models.dev 中提取這些內容。

---

## 疑難排解

如果您在配置供應商時遇到問題，請檢查以下內容：

1. **檢查 auth 設定**：執行 `opencode auth list` 以查看憑證是否
   供應商的配置已添加到您的配置中。

   這不適用於 Amazon Bedrock 等依賴環境變數進行身分驗證的供應商。

2. 對於自定義供應商，請檢查 OpenCode 配置並：
   - 確保 `/connect` 指令中使用的供應商 ID 與 OpenCode 配置中的 ID 匹配。
   - 正確的 npm 套件用於供應商。例如，對 Cerebras 使用 `@ai-sdk/cerebras`。對於所有其他 OpenAI 相容供應商，請使用 `@ai-sdk/openai-compatible`。
   - 檢查 `options.baseURL` 欄位中使用了正確的 API 端點。
