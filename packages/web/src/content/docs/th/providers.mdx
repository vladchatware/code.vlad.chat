---
title: ผู้ให้บริการ
description: ใช้ผู้ให้บริการ LLM ใน OpenCode
---

import config from "../../../../config.mjs"
export const console = config.console

OpenCode ใช้ [AI SDK](https://ai-sdk.dev/) และ [Models.dev](https://models.dev) เพื่อรองรับผู้ให้บริการ **75+ LLM** และรองรับการใช้งานโมเดลท้องถิ่น

หากต้องการเพิ่มผู้ให้บริการ คุณต้อง:

1. เพิ่มคีย์ API สำหรับผู้ให้บริการโดยใช้คำสั่ง `/connect`
2. กำหนดค่าผู้ให้บริการในการกำหนดค่า OpenCode ของคุณ

---

### ข้อมูลรับรอง

เมื่อคุณเพิ่มคีย์ API ของผู้ให้บริการด้วยคำสั่ง `/connect` คีย์เหล่านั้นจะถูกเก็บไว้
ใน `~/.local/share/opencode/auth.json`.

---

### การกำหนดค่า

คุณสามารถปรับแต่งผู้ให้บริการได้ผ่านทางส่วน `provider` ใน OpenCode ของคุณ
กำหนดค่า

---

#### Base URL (URL พื้นฐาน)

คุณสามารถปรับแต่งฐาน URL สำหรับผู้ให้บริการรายใดก็ได้โดยตั้งค่าตัวเลือก `baseURL` สิ่งนี้มีประโยชน์เมื่อใช้บริการพร็อกซีหรือจุดสิ้นสุดแบบกำหนดเอง

```json title="opencode.json" {6}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "anthropic": {
      "options": {
        "baseURL": "https://api.anthropic.com/v1"
      }
    }
  }
}
```

---

## OpenCode Zen

OpenCode Zen คือรายชื่อโมเดลที่จัดทำโดยทีมงาน OpenCode ที่ได้รับ
ทดสอบและตรวจสอบแล้วว่าทำงานได้ดีกับ OpenCode [เรียนรู้เพิ่มเติม](/docs/zen)

:::tip
หากคุณยังใหม่ เราขอแนะนำให้เริ่มต้นด้วย OpenCode Zen
:::

1. เรียกใช้คำสั่ง `/connect` ใน TUI เลือก opencode และไปที่ [opencode.ai/auth](https://opencode.ai/auth)

   ```txt
   /connect
   ```

2. ลงชื่อเข้าใช้ เพิ่มรายละเอียดการเรียกเก็บเงินของคุณ และคัดลอกรหัส API ของคุณ

3. วางคีย์ API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. เรียกใช้ `/models` ใน TUI เพื่อดูรายการรุ่นที่เราแนะนำ

   ```txt
   /models
   ```

มันทำงานเหมือนกับผู้ให้บริการรายอื่นใน OpenCode และเป็นทางเลือกในการใช้งานโดยสมบูรณ์

---

## ไดเรกทอรี

มาดูรายละเอียดผู้ให้บริการบางรายกัน หากคุณต้องการเพิ่มผู้ให้บริการให้กับ
รายการอย่าลังเลที่จะเปิดประชาสัมพันธ์

:::note
ไม่เห็นผู้ให้บริการที่นี่ใช่ไหม ส่งพีอาร์.
:::

---

### 302.AI

1. ไปที่ [302.AI console](https://302.ai/) สร้างบัญชี และสร้างคีย์ API

2. เรียกใช้คำสั่ง `/connect` และค้นหา **302.AI**

   ```txt
   /connect
   ```

3. ป้อนคีย์ 302.AI API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่น

   ```txt
   /models
   ```

---

### Amazon Bedrock

วิธีใช้ Amazon Bedrock กับ OpenCode:

1. ไปที่ **แค็ตตาล็อกโมเดล** ในคอนโซล Amazon Bedrock และขอ
   เข้าถึงโมเดลที่คุณต้องการ

   :::tip
   คุณต้องมีสิทธิ์เข้าถึงโมเดลที่คุณต้องการใน Amazon Bedrock
   :::

2. **กำหนดค่าการตรวจสอบสิทธิ์** โดยใช้วิธีใดวิธีหนึ่งต่อไปนี้:

   #### ตัวแปรสภาพแวดล้อม (เริ่มต้นอย่างรวดเร็ว)

   ตั้งค่าหนึ่งในตัวแปรสภาพแวดล้อมเหล่านี้ขณะเรียกใช้ opencode:

   ```bash
   # Option 1: Using AWS access keys
   AWS_ACCESS_KEY_ID=XXX AWS_SECRET_ACCESS_KEY=YYY opencode

   # Option 2: Using named AWS profile
   AWS_PROFILE=my-profile opencode

   # Option 3: Using Bedrock bearer token
   AWS_BEARER_TOKEN_BEDROCK=XXX opencode
   ```

   หรือเพิ่มลงในโปรไฟล์ทุบตีของคุณ:

   ```bash title="~/.bash_profile"
   export AWS_PROFILE=my-dev-profile
   export AWS_REGION=us-east-1
   ```

   #### ไฟล์การกำหนดค่า (แนะนำ)

   สำหรับการกำหนดค่าเฉพาะโครงการหรือแบบถาวร ให้ใช้ `opencode.json`:

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "my-aws-profile"
         }
       }
     }
   }
   ```

   **ตัวเลือกที่มี:**
   - `region` - ​​AWS ภูมิภาค (เช่น `us-east-1`, `eu-west-1`)
   - `profile` - ​​AWS ตั้งชื่อโปรไฟล์จาก `~/.aws/credentials`
   - `endpoint` - ​​จุดสิ้นสุดที่กำหนดเอง URL สำหรับ VPC จุดสิ้นสุด (นามแฝงสำหรับตัวเลือก `baseURL` ทั่วไป)

   :::tip
   ตัวเลือกไฟล์การกำหนดค่ามีความสำคัญเหนือกว่าตัวแปรสภาพแวดล้อม
   :::

   #### ขั้นสูง: VPC จุดสิ้นสุด

   หากคุณใช้จุดสิ้นสุด VPC สำหรับ Bedrock:

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "production",
           "endpoint": "https://bedrock-runtime.us-east-1.vpce-xxxxx.amazonaws.com"
         }
       }
     }
   }
   ```

   :::note
   ตัวเลือก `endpoint` เป็นนามแฝงสำหรับตัวเลือก `baseURL` ทั่วไป โดยใช้คำศัพท์เฉพาะ AWS หากมีการระบุทั้ง `endpoint` และ `baseURL` `endpoint` จะมีความสำคัญเหนือกว่า
   :::

   #### วิธีการรับรองความถูกต้อง
   - **`AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`**: สร้างผู้ใช้ IAM และสร้างคีย์การเข้าถึงในคอนโซล AWS
   - **`AWS_PROFILE`**: ใช้โปรไฟล์ที่มีชื่อจาก `~/.aws/credentials` ขั้นแรกให้กำหนดค่าด้วย `aws configure --profile my-profile` หรือ `aws sso login`
   - **`AWS_BEARER_TOKEN_BEDROCK`**: สร้างคีย์ API ระยะยาวจากคอนโซล Amazon Bedrock
   - **`AWS_WEB_IDENTITY_TOKEN_FILE` / `AWS_ROLE_ARN`**: สำหรับ EKS IRSA (IAM บทบาทสำหรับบัญชีบริการ) หรือสภาพแวดล้อม Kubernetes อื่นๆ ที่มีการเชื่อมโยง OIDC Kubernetes แทรกตัวแปรสภาพแวดล้อมเหล่านี้โดยอัตโนมัติเมื่อใช้คำอธิบายประกอบของบัญชีบริการ

   #### ลำดับความสำคัญของการรับรองความถูกต้อง

   Amazon Bedrock ใช้ลำดับความสำคัญในการรับรองความถูกต้องต่อไปนี้:
   1. **Bearer Token** - `AWS_BEARER_TOKEN_BEDROCK` ตัวแปรสภาพแวดล้อมหรือโทเค็นจากคำสั่ง `/connect`
   2. **AWS Credential Chain** - โปรไฟล์, คีย์การเข้าถึง, ข้อมูลประจำตัวที่แชร์, บทบาท IAM, โทเค็นข้อมูลประจำตัวของเว็บ (EKS IRSA), ข้อมูลเมตาของอินสแตนซ์

   :::note
   เมื่อตั้งค่าBearer Token (ผ่าน `/connect` หรือ `AWS_BEARER_TOKEN_BEDROCK`) โทเค็นนั้นจะมีความสำคัญเหนือกว่าวิธีการข้อมูลประจำตัว AWS ทั้งหมด รวมถึงโปรไฟล์ที่กำหนดค่าไว้
   :::

3. รันคำสั่ง `/models` เพื่อเลือกรุ่นที่ต้องการ

   ```txt
   /models
   ```

:::note
สำหรับโปรไฟล์การอนุมานแบบกำหนดเอง ให้ใช้ชื่อรุ่นและผู้ให้บริการในคีย์และตั้งค่าคุณสมบัติ `id` เป็น arn สิ่งนี้ทำให้มั่นใจได้ว่าแคชถูกต้อง:

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "amazon-bedrock": {
      // ...
      "models": {
        "anthropic-claude-sonnet-4.5": {
          "id": "arn:aws:bedrock:us-east-1:xxx:application-inference-profile/yyy"
        }
      }
    }
  }
}
```

:::

---

### Anthropic

1. เมื่อคุณสมัครแล้ว ให้รันคำสั่ง `/connect` และเลือก Anthropic

   ```txt
   /connect
   ```

2. ที่นี่คุณสามารถเลือกตัวเลือก **Claude Pro/Max** และมันจะเปิดเบราว์เซอร์ของคุณ
   และขอให้คุณตรวจสอบสิทธิ์

   ```txt
   ┌ Select auth method
   │
   │ Claude Pro/Max
   │ Create an API Key
   │ Manually enter API Key
   └
   ```

3. ตอนนี้โมเดล Anthropic ทั้งหมดควรจะพร้อมใช้งานเมื่อคุณใช้คำสั่ง `/models`

   ```txt
   /models
   ```

:::info
การใช้การสมัครสมาชิก Claude Pro/Max ใน OpenCode ไม่ได้รับการสนับสนุนอย่างเป็นทางการจาก [Anthropic](https://anthropic.com)
:::

##### การใช้ API Key

คุณยังสามารถเลือก **สร้าง API คีย์** หากคุณไม่มีการสมัครสมาชิก Pro/Max นอกจากนี้ยังจะเปิดเบราว์เซอร์ของคุณและขอให้คุณเข้าสู่ระบบ Anthropic และให้รหัสที่คุณสามารถวางใน terminal ของคุณ

หรือหากคุณมีคีย์ API อยู่แล้ว คุณสามารถเลือก **ป้อนคีย์ API ด้วยตนเอง** และวางลงใน terminal ของคุณ

---

### Azure OpenAI

:::note
หากคุณพบข้อผิดพลาด "ฉันขอโทษ แต่ฉันไม่สามารถช่วยเหลือคำขอนั้นได้" ให้ลองเปลี่ยนตัวกรองเนื้อหาจาก **DefaultV2** เป็น **Default** ในทรัพยากร Azure ของคุณ
:::

1. ไปที่ [พอร์ทัล Azure](https://portal.azure.com/) และสร้างทรัพยากร **Azure OpenAI** คุณจะต้องการ:
   - **ชื่อทรัพยากร**: นี่จะกลายเป็นส่วนหนึ่งของจุดสิ้นสุด API ของคุณ (`https://RESOURCE_NAME.openai.azure.com/`)
   - **API key**: `KEY 1` หรือ `KEY 2` จากทรัพยากรของคุณ

2. ไปที่ [Azure AI Foundry](https://ai.azure.com/) และปรับใช้โมเดล

   :::note
   ชื่อการปรับใช้จะต้องตรงกับชื่อรุ่นเพื่อให้ opencode ทำงานได้อย่างถูกต้อง
   :::

3. เรียกใช้คำสั่ง `/connect` และค้นหา **Azure**

   ```txt
   /connect
   ```

4. ป้อน API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. ตั้งชื่อทรัพยากรของคุณเป็นตัวแปรสภาพแวดล้อม:

   ```bash
   AZURE_RESOURCE_NAME=XXX opencode
   ```

   หรือเพิ่มลงในโปรไฟล์ทุบตีของคุณ:

   ```bash title="~/.bash_profile"
   export AZURE_RESOURCE_NAME=XXX
   ```

6. รันคำสั่ง `/models` เพื่อเลือกโมเดลที่ปรับใช้ของคุณ

   ```txt
   /models
   ```

---

### Azure Cognitive Services

1. ไปที่ [พอร์ทัล Azure](https://portal.azure.com/) และสร้างทรัพยากร **Azure OpenAI** คุณจะต้องการ:
   - **ชื่อทรัพยากร**: นี่จะกลายเป็นส่วนหนึ่งของจุดสิ้นสุด API ของคุณ (`https://AZURE_COGNITIVE_SERVICES_RESOURCE_NAME.cognitiveservices.azure.com/`)
   - **API key**: `KEY 1` หรือ `KEY 2` จากทรัพยากรของคุณ

2. ไปที่ [Azure AI Foundry](https://ai.azure.com/) และปรับใช้โมเดล

   :::note
   ชื่อการปรับใช้จะต้องตรงกับชื่อรุ่นเพื่อให้ opencode ทำงานได้อย่างถูกต้อง
   :::

3. เรียกใช้คำสั่ง `/connect` และค้นหา **Azure Cognitive Services**

   ```txt
   /connect
   ```

4. ป้อน API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. ตั้งชื่อทรัพยากรของคุณเป็นตัวแปรสภาพแวดล้อม:

   ```bash
   AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX opencode
   ```

   หรือเพิ่มลงในโปรไฟล์ทุบตีของคุณ:

   ```bash title="~/.bash_profile"
   export AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX
   ```

6. รันคำสั่ง `/models` เพื่อเลือกโมเดลที่ปรับใช้ของคุณ

   ```txt
   /models
   ```

---

### Baseten

1. ไปที่ [Baseten](https://app.baseten.co/) สร้างบัญชี และสร้างคีย์ API

2. รันคำสั่ง `/connect` และค้นหา **Baseten**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Baseten API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่น

   ```txt
   /models
   ```

---

### Cerebras

1. ไปที่ [คอนโซล Cerebras](https://inference.cerebras.ai/) สร้างบัญชี และสร้างคีย์ API

2. เรียกใช้คำสั่ง `/connect` และค้นหา **Cerebras**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Cerebras API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่นเช่น _Qwen 3 Coder 480B_

   ```txt
   /models
   ```

---

### Cloudflare AI Gateway

Cloudflare AI Gateway ช่วยให้คุณเข้าถึงโมเดลจาก OpenAI, Anthropic, Workers AI และอื่นๆ อีกมากมายผ่านตำแหน่งข้อมูลแบบรวม ด้วย [การเรียกเก็บเงินแบบรวม](https://developers.cloudflare.com/ai-gateway/features/unified-billing/) คุณไม่จำเป็นต้องใช้คีย์ API แยกต่างหากสำหรับผู้ให้บริการแต่ละราย

1. ไปที่ [แดชบอร์ด Cloudflare](https://dash.cloudflare.com/) ไปที่ **AI** > **AI Gateway** และสร้างเกตเวย์ใหม่

2. ตั้งค่ารหัสบัญชีและรหัสเกตเวย์ของคุณเป็นตัวแปรสภาพแวดล้อม

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_ACCOUNT_ID=your-32-character-account-id
   export CLOUDFLARE_GATEWAY_ID=your-gateway-id
   ```

3. รันคำสั่ง `/connect` และค้นหา **Cloudflare AI Gateway**

   ```txt
   /connect
   ```

4. ป้อน Cloudflare API Token ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

   หรือตั้งเป็นตัวแปรสภาพแวดล้อม

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_API_TOKEN=your-api-token
   ```

5. รันคำสั่ง `/models` เพื่อเลือกรุ่น

   ```txt
   /models
   ```

   คุณยังสามารถเพิ่มโมเดลผ่านการกำหนดค่า opencode ของคุณได้

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "cloudflare-ai-gateway": {
         "models": {
           "openai/gpt-4o": {},
           "anthropic/claude-sonnet-4": {}
         }
       }
     }
   }
   ```

---

### Cortecs

1. ไปที่ [คอนโซล Cortecs](https://cortecs.ai/) สร้างบัญชี และสร้างคีย์ API

2. เรียกใช้คำสั่ง `/connect` และค้นหา **Cortecs**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Cortecs API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดลเช่น _Kimi K2 Instruct_

   ```txt
   /models
   ```

---

### DeepSeek

1. ไปที่ [คอนโซล DeepSeek](https://platform.deepseek.com/) สร้างบัญชี แล้วคลิก **สร้างคีย์ API ใหม่**

2. รันคำสั่ง `/connect` และค้นหา **DeepSeek**

   ```txt
   /connect
   ```

3. ป้อนคีย์ DeepSeek API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดล DeepSeek เช่น _DeepSeek Reasoner_

   ```txt
   /models
   ```

---

### Deep Infra

1. ไปที่ [แดชบอร์ด Deep Infra](https://deepinfra.com/dash) สร้างบัญชี และสร้างคีย์ API

2. เรียกใช้คำสั่ง `/connect` และค้นหา **Deep Infra**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Deep Infra API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่น

   ```txt
   /models
   ```

---

### Firmware

1. ไปที่ [แดชบอร์ด Firmware](https://app.firmware.ai/signup) สร้างบัญชี และสร้างคีย์ API

2. เรียกใช้คำสั่ง `/connect` และค้นหา **Firmware**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Firmware API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่น

   ```txt
   /models
   ```

---

### Fireworks AI

1. ไปที่ [คอนโซล Fireworks AI](https://app.fireworks.ai/) สร้างบัญชี แล้วคลิก **Create API Key**

2. รันคำสั่ง `/connect` และค้นหา **Fireworks AI**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Fireworks AI API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดลเช่น _Kimi K2 Instruct_

   ```txt
   /models
   ```

---

### GitLab Duo

GitLab Duo ให้การแชทแบบเอเจนต์ที่ขับเคลื่อนด้วย AI พร้อมความสามารถในการเรียกใช้เครื่องมือแบบเนทีฟผ่านพร็อกซี Anthropic ของ GitLab

1. รันคำสั่ง `/connect` และเลือก GitLab

   ```txt
   /connect
   ```

2. เลือกวิธีการรับรองความถูกต้องของคุณ:

   ```txt
   ┌ Select auth method
   │
   │ OAuth (Recommended)
   │ Personal Access Token
   └
   ```

   #### การใช้ OAuth (แนะนำ)

   เลือก **OAuth** และเบราว์เซอร์ของคุณจะเปิดขึ้นเพื่อรับสิทธิ์

#### การใช้ Personal Access Token

1.  ไปที่ [GitLab User Settings > Access Tokens](https://gitlab.com/-/user_settings/personal_access_tokens)
2.  คลิก **เพิ่มโทเค็นใหม่**
3.  ชื่อ: `OpenCode`, ขอบเขต: `api`
4.  คัดลอกโทเค็น (เริ่มต้นด้วย `glpat-`)
5.  ป้อนลงใน terminal

6.  รันคำสั่ง `/models` เพื่อดูรุ่นที่มีจำหน่าย

    ```txt
    /models
    ```

    มีโมเดลที่ใช้ Claude สามรุ่นให้เลือก:
    - **duo-chat-haiku-4-5** (ค่าเริ่มต้น) - ตอบสนองรวดเร็วสำหรับงานด่วน
    - **duo-chat-sonnet-4-5** - ประสิทธิภาพที่สมดุลสำหรับขั้นตอนการทำงานส่วนใหญ่
    - **duo-chat-opus-4-5** - มีความสามารถมากที่สุดสำหรับการวิเคราะห์ที่ซับซ้อน

:::note
คุณยังสามารถระบุตัวแปรสภาพแวดล้อม 'GITLAB_TOKEN' ได้หากคุณไม่ต้องการ
เพื่อจัดเก็บโทเค็นในที่เก็บข้อมูลการรับรองความถูกต้องของ opencode
:::

##### GitLab ที่โฮสต์เอง

:::note[บันทึกการปฏิบัติตาม]
OpenCode ใช้โมเดลขนาดเล็กสำหรับงาน AI บางอย่าง เช่น การสร้างชื่อเซสชัน
มีการกำหนดค่าให้ใช้ gpt-5-nano ตามค่าเริ่มต้น ซึ่งโฮสต์โดย Zen เพื่อล็อค OpenCode
หากต้องการใช้เฉพาะอินสแตนซ์ที่โฮสต์บน GitLab ของคุณเอง ให้เพิ่มสิ่งต่อไปนี้ในของคุณ
`opencode.json` ไฟล์. ขอแนะนำให้ปิดใช้งานการแชร์เซสชันด้วย

```json
{
  "$schema": "https://opencode.ai/config.json",
  "small_model": "gitlab/duo-chat-haiku-4-5",
  "share": "disabled"
}
```

:::

สำหรับอินสแตนซ์ GitLab ที่โฮสต์เอง:

```bash
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_TOKEN=glpat-...
```

หากอินสแตนซ์ของคุณใช้งาน AI Gateway แบบกำหนดเอง:

```bash
GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
```

หรือเพิ่มในโปรไฟล์ทุบตีของคุณ:

```bash title="~/.bash_profile"
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
export GITLAB_TOKEN=glpat-...
```

:::note
ผู้ดูแลระบบ GitLab ของคุณต้องเปิดใช้งานสิ่งต่อไปนี้:

1. [Duo Agent Platform](https://docs.gitlab.com/user/gitlab_duo/turn_on_off/) สำหรับผู้ใช้ กลุ่ม หรืออินสแตนซ์
2. แฟล็กคุณลักษณะ (ผ่านคอนโซล Rails):
   - `agent_platform_claude_code`
   - `third_party_agents_enabled`
     :::

##### OAuth สำหรับอินสแตนซ์ที่โฮสต์เอง

คุณต้องสร้างเพื่อให้ Oauth ทำงานกับอินสแตนซ์ที่โฮสต์เองได้
แอพพลิเคชั่นใหม่ (Settings → Applications) ด้วย
โทรกลับ URL `http://127.0.0.1:8080/callback` และขอบเขตต่อไปนี้:

- api (เข้าถึง API ในนามของคุณ)
- read_user (อ่านข้อมูลส่วนตัวของคุณ)
- read_repository (อนุญาตให้เข้าถึงที่เก็บแบบอ่านอย่างเดียว)

จากนั้นเปิดเผย ID แอปพลิเคชันเป็นตัวแปรสภาพแวดล้อม:

```bash
export GITLAB_OAUTH_CLIENT_ID=your_application_id_here
```

เอกสารประกอบเพิ่มเติมเกี่ยวกับหน้าแรกของ [opencode-gitlab-auth](https://www.npmjs.com/package/@gitlab/opencode-gitlab-auth)

##### การกำหนดค่า

ปรับแต่งผ่าน `opencode.json`:

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "gitlab": {
      "options": {
        "instanceUrl": "https://gitlab.com",
        "featureFlags": {
          "duo_agent_platform_agentic_chat": true,
          "duo_agent_platform": true
        }
      }
    }
  }
}
```

##### GitLab API Tools (เป็นทางเลือก แต่แนะนำเป็นอย่างยิ่ง)

วิธีเข้าถึงเครื่องมือ GitLab (รวมคำขอ ปัญหา ไปป์ไลน์ CI/CD ฯลฯ):

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "plugin": ["@gitlab/opencode-gitlab-plugin"]
}
```

ปลั๊กอินนี้มอบความสามารถในการจัดการพื้นที่เก็บข้อมูล GitLab ที่ครอบคลุม รวมถึงการตรวจสอบ MR การติดตามปัญหา การตรวจสอบไปป์ไลน์ และอื่นๆ

---

### GitHub Copilot

วิธีใช้การสมัครสมาชิก GitHub Copilot ด้วย opencode:

:::note
บางรุ่นอาจต้องมี [Pro+
subscription](https://github.com/features/copilot/plans) เพื่อใช้งาน

บางรุ่นจำเป็นต้องเปิดใช้งานด้วยตนเองใน [การตั้งค่า GitHub Copilot](https://docs.github.com/en/copilot/how-tos/use-ai-models/configure-access-to-ai-models#setup-for-individual-use)
:::

1. รันคำสั่ง `/connect` และค้นหา GitHub Copilot

   ```txt
   /connect
   ```

2. ไปที่ [github.com/login/device](https://github.com/login/device) และป้อนรหัส

   ```txt
   ┌ Login with GitHub Copilot
   │
   │ https://github.com/login/device
   │
   │ Enter code: 8F43-6FCF
   │
   │ Waiting for authorization...
   ```

3. ตอนนี้รันคำสั่ง `/models` เพื่อเลือกรุ่นที่คุณต้องการ

   ```txt
   /models
   ```

---

### Google Vertex AI

วิธีใช้ Google Vertex AI กับ OpenCode:

1. ไปที่ **Model Garden** ใน Google Cloud Console แล้วตรวจสอบ
   รุ่นที่มีในภูมิภาคของคุณ

   :::note
   คุณต้องมีโปรเจ็กต์ Google Cloud ที่เปิดใช้งาน Vertex AI API
   :::

2. ตั้งค่าตัวแปรสภาพแวดล้อมที่ต้องการ:
   - `GOOGLE_CLOUD_PROJECT`: รหัสโครงการ Google Cloud ของคุณ
   - `VERTEX_LOCATION` (เป็นทางเลือก): ภูมิภาคสำหรับ Vertex AI (ค่าเริ่มต้นคือ `global`)
   - การรับรองความถูกต้อง (เลือกหนึ่งรายการ):
     - `GOOGLE_APPLICATION_CREDENTIALS`: เส้นทางไปยังบัญชีบริการของคุณ JSON ไฟล์คีย์
     - ตรวจสอบสิทธิ์โดยใช้ gcloud CLI: `gcloud auth application-default login`

   ตั้งค่าในขณะที่เรียกใช้ opencode

   ```bash
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json GOOGLE_CLOUD_PROJECT=your-project-id opencode
   ```

   หรือเพิ่มลงในโปรไฟล์ทุบตีของคุณ

   ```bash title="~/.bash_profile"
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
   export GOOGLE_CLOUD_PROJECT=your-project-id
   export VERTEX_LOCATION=global
   ```

:::tip
ภูมิภาค `global` ปรับปรุงความพร้อมใช้งานและลดข้อผิดพลาดโดยไม่มีค่าใช้จ่ายเพิ่มเติม ใช้ตำแหน่งข้อมูลระดับภูมิภาค (เช่น `us-central1`) สำหรับข้อกำหนดด้านถิ่นที่อยู่ของข้อมูล [เรียนรู้เพิ่มเติม](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#regional_and_global_endpoints)
:::

3. รันคำสั่ง `/models` เพื่อเลือกรุ่นที่ต้องการ

   ```txt
   /models
   ```

---

### Groq

1. ไปที่ [Groq console](https://console.groq.com/) คลิก **Create API Key** และคัดลอกคีย์

2. เรียกใช้คำสั่ง `/connect` และค้นหา Groq

   ```txt
   /connect
   ```

3. ป้อนคีย์ API สำหรับผู้ให้บริการ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกคำสั่งที่คุณต้องการ

   ```txt
   /models
   ```

---

### Hugging Face

[ผู้ให้บริการการอนุมาน Hugging Face](https://huggingface.co/docs/inference-providers) ช่วยให้สามารถเข้าถึงโมเดลแบบเปิดที่รองรับโดยผู้ให้บริการมากกว่า 17 ราย

1. ไปที่ [การตั้งค่า Hugging Face](https://huggingface.co/settings/tokens/new?ownUserPermissions=inference.serverless.write&tokenType=fineGrained) เพื่อสร้างโทเค็นที่มีสิทธิ์ในการโทรไปยังผู้ให้บริการการอนุมาน

2. เรียกใช้คำสั่ง `/connect` และค้นหา **Hugging Face**

   ```txt
   /connect
   ```

3. ป้อน Hugging Face Token ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดล เช่น _Kimi-K2-Instruct_ หรือ _GLM-4.6_

   ```txt
   /models
   ```

---

### Helicone

[Helicone](https://helicone.ai) เป็นแพลตฟอร์มการติดตาม LLM ที่ให้การบันทึก การตรวจสอบ และการวิเคราะห์สำหรับแอปพลิเคชัน AI ของคุณ Helicone AI Gateway กำหนดเส้นทางคำขอของคุณไปยังผู้ให้บริการที่เหมาะสมโดยอัตโนมัติตามรุ่น

1. ไปที่ [Helicone](https://helicone.ai) สร้างบัญชี และสร้างคีย์ API จากแดชบอร์ดของคุณ

2. รันคำสั่ง `/connect` และค้นหา **Helicone**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Helicone API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่น

   ```txt
   /models
   ```

หากต้องการทราบผู้ให้บริการเพิ่มเติมและฟีเจอร์ขั้นสูง เช่น การแคชและการจำกัดอัตรา โปรดตรวจสอบ [เอกสารประกอบของ Helicone](https://docs.helicone.ai)

#### การกำหนดค่าเพิ่มเติม

ในกรณีที่คุณเห็นคุณสมบัติหรือรุ่นจาก Helicone ที่ไม่ได้กำหนดค่าโดยอัตโนมัติผ่าน opencode คุณสามารถกำหนดค่าด้วยตนเองได้ตลอดเวลา

นี่คือ [Helicone's Model Directory](https://helicone.ai/models) คุณจะต้องใช้สิ่งนี้เพื่อรับ ID ของโมเดลที่คุณต้องการเพิ่ม

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
      },
      "models": {
        "gpt-4o": {
          // Model ID (from Helicone's model directory page)
          "name": "GPT-4o", // Your own custom name for the model
        },
        "claude-sonnet-4-20250514": {
          "name": "Claude Sonnet 4",
        },
      },
    },
  },
}
```

#### ส่วนหัวที่กำหนดเอง

Helicone รองรับส่วนหัวแบบกำหนดเองสำหรับฟีเจอร์ต่างๆ เช่น การแคช การติดตามผู้ใช้ และการจัดการเซสชัน เพิ่มลงในการกำหนดค่าผู้ให้บริการของคุณโดยใช้ `options.headers`:

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
        "headers": {
          "Helicone-Cache-Enabled": "true",
          "Helicone-User-Id": "opencode",
        },
      },
    },
  },
}
```

##### การติดตามเซสชัน

คุณลักษณะ [เซสชัน](https://docs.helicone.ai/features/sessions) ของ Helicone ช่วยให้คุณสามารถจัดกลุ่มคำขอ LLM ที่เกี่ยวข้องเข้าด้วยกันได้ ใช้ปลั๊กอิน [opencode-helicone-session](https://github.com/H2Shami/opencode-helicone-session) เพื่อบันทึกการสนทนา OpenCode แต่ละรายการเป็นเซสชันใน Helicone โดยอัตโนมัติ

```bash
npm install -g opencode-helicone-session
```

เพิ่มลงในการกำหนดค่าของคุณ

```json title="opencode.json"
{
  "plugin": ["opencode-helicone-session"]
}
```

ปลั๊กอินจะแทรกส่วนหัว `Helicone-Session-Id` และ `Helicone-Session-Name` ลงในคำขอของคุณ ในหน้าเซสชันของ Helicone คุณจะเห็นการสนทนา OpenCode แต่ละรายการแยกเป็นเซสชันแยกกัน

##### ส่วนหัวของ Helicone ทั่วไป

| ส่วนหัว                    | คำอธิบาย                                                         |
| -------------------------- | ---------------------------------------------------------------- |
| `Helicone-Cache-Enabled`   | เปิดใช้งานการแคชการตอบกลับ (`true`/`false`)                      |
| `Helicone-User-Id`         | ติดตามตัวชี้วัดตามผู้ใช้                                         |
| `Helicone-Property-[Name]` | เพิ่มคุณสมบัติที่กำหนดเอง (เช่น `Helicone-Property-Environment`) |
| `Helicone-Prompt-Id`       | เชื่อมโยงคำขอกับเวอร์ชันพร้อมท์                                  |

ดู [Helicone Header Directory](https://docs.helicone.ai/helicone-headers/header-directory) สำหรับส่วนหัวที่มีอยู่ทั้งหมด

---

### llama.cpp

คุณสามารถกำหนดค่า opencode เพื่อใช้โมเดลท้องถิ่นผ่านยูทิลิตี้ [llama.cpp's](https://github.com/ggml-org/llama.cpp) llama-server

```json title="opencode.json" "llama.cpp" {5, 6, 8, 10-15}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "llama.cpp": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "llama-server (local)",
      "options": {
        "baseURL": "http://127.0.0.1:8080/v1"
      },
      "models": {
        "qwen3-coder:a3b": {
          "name": "Qwen3-Coder: a3b-30b (local)",
          "limit": {
            "context": 128000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

ในตัวอย่างนี้:

- `llama.cpp` คือ ID ผู้ให้บริการที่กำหนดเอง นี่อาจเป็นสตริงใดก็ได้ที่คุณต้องการ
- `npm` ระบุแพ็คเกจที่จะใช้สำหรับผู้ให้บริการรายนี้ ในที่นี้ `@ai-sdk/openai-compatible` ใช้สำหรับ API ที่เข้ากันได้กับ OpenAI
- `name` คือชื่อที่แสดงของผู้ให้บริการใน UI
- `options.baseURL` เป็นจุดสิ้นสุดสำหรับเซิร์ฟเวอร์ภายในเครื่อง
- `models` คือแมปของ ID โมเดลกับการกำหนดค่า ชื่อรุ่นจะปรากฏในรายการเลือกรุ่น

---

### IO.NET

IO.NET มี 17 โมเดลที่ได้รับการปรับให้เหมาะกับการใช้งานที่หลากหลาย:

1. ไปที่ [IO.NET console](https://ai.io.net/) สร้างบัญชี และสร้างคีย์ API

2. รันคำสั่ง `/connect` และค้นหา **IO.NET**

   ```txt
   /connect
   ```

3. ป้อนคีย์ IO.NET API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่น

   ```txt
   /models
   ```

---

### LM Studio

คุณสามารถกำหนดค่า opencode เพื่อใช้โมเดลท้องถิ่นผ่าน LM Studio

```json title="opencode.json" "lmstudio" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "lmstudio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (local)",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1"
      },
      "models": {
        "google/gemma-3n-e4b": {
          "name": "Gemma 3n-e4b (local)"
        }
      }
    }
  }
}
```

ในตัวอย่างนี้:

- `lmstudio` คือ ID ผู้ให้บริการที่กำหนดเอง นี่อาจเป็นสตริงใดก็ได้ที่คุณต้องการ
- `npm` ระบุแพ็คเกจที่จะใช้สำหรับผู้ให้บริการรายนี้ ในที่นี้ `@ai-sdk/openai-compatible` ใช้สำหรับ API ที่เข้ากันได้กับ OpenAI
- `name` คือชื่อที่แสดงของผู้ให้บริการใน UI
- `options.baseURL` เป็นจุดสิ้นสุดสำหรับเซิร์ฟเวอร์ภายในเครื่อง
- `models` คือแมปของ ID โมเดลกับการกำหนดค่า ชื่อรุ่นจะปรากฏในรายการเลือกรุ่น

---

### Moonshot AI

วิธีใช้ Kimi K2 จาก Moonshot AI:

1. ไปที่ [Moonshot AI console](https://platform.moonshot.ai/console) สร้างบัญชี แล้วคลิก **Create API Key**

2. รันคำสั่ง `/connect` และค้นหา **Moonshot AI**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Moonshot API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือก _Kimi K2_

   ```txt
   /models
   ```

---

### MiniMax

1. ไปที่ [MiniMax API Console](https://platform.minimax.io/login) สร้างบัญชี และสร้างคีย์ API

2. เรียกใช้คำสั่ง `/connect` และค้นหา **MiniMax**

   ```txt
   /connect
   ```

3. ป้อนคีย์ MiniMax API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่นเช่น _M2.1_

   ```txt
   /models
   ```

---

### Nebius Token Factory

1. ไปที่ [คอนโซล Nebius Token Factory](https://tokenfactory.nebius.com/) สร้างบัญชี แล้วคลิก **Add key**

2. รันคำสั่ง `/connect` และค้นหา **Nebius Token Factory**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Nebius Token Factory API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดลเช่น _Kimi K2 Instruct_

   ```txt
   /models
   ```

---

### Ollama

คุณสามารถกำหนดค่า opencode เพื่อใช้โมเดลท้องถิ่นผ่าน Ollama

:::tip
Ollama สามารถกำหนดค่าตัวเองสำหรับ OpenCode ได้โดยอัตโนมัติ ดู [เอกสารการบูรณาการ Ollama](https://docs.ollama.com/integrations/opencode) เพื่อดูรายละเอียด
:::

```json title="opencode.json" "ollama" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (local)",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "llama2": {
          "name": "Llama 2"
        }
      }
    }
  }
}
```

ในตัวอย่างนี้:

- `ollama` คือ ID ผู้ให้บริการที่กำหนดเอง นี่อาจเป็นสตริงใดก็ได้ที่คุณต้องการ
- `npm` ระบุแพ็คเกจที่จะใช้สำหรับผู้ให้บริการรายนี้ ในที่นี้ `@ai-sdk/openai-compatible` ใช้สำหรับ API ที่เข้ากันได้กับ OpenAI
- `name` คือชื่อที่แสดงของผู้ให้บริการใน UI
- `options.baseURL` เป็นจุดสิ้นสุดสำหรับเซิร์ฟเวอร์ภายในเครื่อง
- `models` คือแมปของ ID โมเดลกับการกำหนดค่า ชื่อรุ่นจะปรากฏในรายการเลือกรุ่น

:::tip
หากการเรียกใช้เครื่องมือไม่ทำงาน ให้ลองเพิ่ม `num_ctx` ใน Ollama เริ่มต้นประมาณ 16,000 - 32,000
:::

---

### Ollama Cloud

วิธีใช้ Ollama Cloud กับ OpenCode:

1. ไปที่ [https://ollama.com/](https://ollama.com/) แล้วลงชื่อเข้าใช้หรือสร้างบัญชี

2. ไปที่ **Settings** > **Keys** แล้วคลิก **Add API Key** เพื่อสร้างคีย์ API ใหม่

3. คัดลอกคีย์ API เพื่อใช้ใน OpenCode

4. เรียกใช้คำสั่ง `/connect` และค้นหา **Ollama Cloud**

   ```txt
   /connect
   ```

5. ป้อนคีย์ Ollama Cloud API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

6. **สำคัญ**: ก่อนที่จะใช้โมเดลคลาวด์ใน OpenCode คุณต้องดึงข้อมูลโมเดลในเครื่อง:

   ```bash
   ollama pull gpt-oss:20b-cloud
   ```

7. รันคำสั่ง `/models` เพื่อเลือกโมเดล Ollama Cloud ของคุณ

   ```txt
   /models
   ```

---

### OpenAI

เราขอแนะนำให้สมัคร [ChatGPT Plus หรือ Pro](https://chatgpt.com/pricing)

1. เมื่อคุณสมัครแล้ว ให้รันคำสั่ง `/connect` และเลือก OpenAI

   ```txt
   /connect
   ```

2. ที่นี่คุณสามารถเลือกตัวเลือก **ChatGPT Plus/Pro** จากนั้นเบราว์เซอร์จะเปิดขึ้นมา
   และขอให้คุณตรวจสอบสิทธิ์

   ```txt
   ┌ Select auth method
   │
   │ ChatGPT Plus/Pro
   │ Manually enter API Key
   └
   ```

3. ตอนนี้โมเดล OpenAI ทั้งหมดควรจะพร้อมใช้งานเมื่อคุณใช้คำสั่ง `/models`

   ```txt
   /models
   ```

##### การใช้ API Key

หากคุณมีคีย์ API อยู่แล้ว คุณสามารถเลือก **Manually enter API Key** และวางลงใน terminal ของคุณ

---

### OpenCode Zen

OpenCode Zen คือรายการโมเดลที่ได้รับการทดสอบและตรวจสอบแล้วโดยทีมงาน OpenCode [เรียนรู้เพิ่มเติม](/docs/zen)

1. ลงชื่อเข้าใช้ **<a href={console}>OpenCode Zen</a>** และคลิก **สร้าง API คีย์**

2. เรียกใช้คำสั่ง `/connect` และค้นหา **OpenCode Zen**

   ```txt
   /connect
   ```

3. ป้อนคีย์ OpenCode API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่นเช่น _Qwen 3 Coder 480B_

   ```txt
   /models
   ```

---

### เปิดเราเตอร์

1. ไปที่ [แดชบอร์ด OpenRouter](https://openrouter.ai/settings/keys) คลิก **สร้าง API คีย์** และคัดลอกคีย์

2. เรียกใช้คำสั่ง `/connect` และค้นหา OpenRouter

   ```txt
   /connect
   ```

3. ป้อนคีย์ API สำหรับผู้ให้บริการ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. OpenRouter หลายรุ่นถูกโหลดไว้ล่วงหน้าตามค่าเริ่มต้น ให้รันคำสั่ง `/models` เพื่อเลือกรุ่นที่คุณต้องการ

   ```txt
   /models
   ```

   คุณยังสามารถเพิ่มโมเดลเพิ่มเติมผ่านการกำหนดค่า opencode ของคุณ

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

5. คุณยังสามารถปรับแต่งได้ผ่านการกำหนดค่า opencode ของคุณ นี่คือตัวอย่างการระบุผู้ให้บริการ

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "moonshotai/kimi-k2": {
             "options": {
               "provider": {
                 "order": ["baseten"],
                 "allow_fallbacks": false
               }
             }
           }
         }
       }
     }
   }
   ```

---

### SAP เอไอคอร์

SAP AI Core ให้การเข้าถึงโมเดลมากกว่า 40 โมเดลจาก OpenAI, Anthropic, Google, Amazon, Meta, Mistral และ AI21 ผ่านแพลตฟอร์มแบบครบวงจร

1. ไปที่ห้องนักบิน [SAP BTP](https://account.hana.ondemand.com/) นำทางไปยังอินสแตนซ์บริการ SAP AI Core ของคุณ และสร้างรหัสบริการ

   :::tip
   รหัสบริการคือวัตถุ JSON ที่ประกอบด้วย `clientid`, `clientsecret`, `url` และ `serviceurls.AI_API_URL` คุณสามารถค้นหาอินสแตนซ์ AI Core ของคุณได้ภายใต้ **บริการ** > **อินสแตนซ์และการสมัครสมาชิก** ในห้องควบคุม BTP
   :::

2. รันคำสั่ง `/connect` และค้นหา **SAP AI Core**

   ```txt
   /connect
   ```

3. ป้อนรหัสบริการของคุณ JSON

   ```txt
   ┌ Service key
   │
   │
   └ enter
   ```

   หรือตั้งค่าตัวแปรสภาพแวดล้อม `AICORE_SERVICE_KEY`:

   ```bash
   AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}' opencode
   ```

   หรือเพิ่มลงในโปรไฟล์ทุบตีของคุณ:

   ```bash title="~/.bash_profile"
   export AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}'
   ```

4. ตั้งค่า ID การนำไปใช้งานและกลุ่มทรัพยากรเพิ่มเติม:

   ```bash
   AICORE_DEPLOYMENT_ID=your-deployment-id AICORE_RESOURCE_GROUP=your-resource-group opencode
   ```

   :::note
   การตั้งค่าเหล่านี้เป็นทางเลือกและควรกำหนดค่าตามการตั้งค่า SAP AI Core ของคุณ
   :::

5. รันคำสั่ง `/models` เพื่อเลือกจากรุ่นที่มีให้เลือกมากกว่า 40 รุ่น

   ```txt
   /models
   ```

---

### ตำแหน่งข้อมูล AI ของ OVHcloud

1. ไปที่ [แผง OVHcloud](https://ovh.com/manager) ไปที่ส่วน `Public Cloud` `AI & Machine Learning` > `AI Endpoints` และในแท็บ `API Keys` คลิก **สร้างคีย์ API ใหม่**

2. รันคำสั่ง `/connect` และค้นหา **OVHcloud AI Endpoints**

   ```txt
   /connect
   ```

3. ป้อนคีย์ OVHcloud AI Endpoints API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดลเช่น _gpt-oss-120b_

   ```txt
   /models
   ```

---

### สเกลเวย์

วิธีใช้ [Scaleway Generative API](https://www.scaleway.com/en/docs/generative-apis/) กับ Opencode:

1. ไปที่ [การตั้งค่า Scaleway Console IAM](https://console.scaleway.com/iam/api-keys) เพื่อสร้างคีย์ API ใหม่

2. รันคำสั่ง `/connect` และค้นหา **Scaleway**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Scaleway API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดล เช่น _devstral-2-123b-instruct-2512_ หรือ _gpt-oss-120b_

   ```txt
   /models
   ```

---

### ร่วมกันเอไอ

1. ไปที่ [Together AI console](https://api.together.ai) สร้างบัญชี แล้วคลิก **เพิ่มรหัส**

2. เรียกใช้คำสั่ง `/connect` และค้นหา **Together AI**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Together AI API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดลเช่น _Kimi K2 Instruct_

   ```txt
   /models
   ```

---

### เวนิส เอไอ

1. ไปที่ [คอนโซล AI ของเวนิส](https://venice.ai) สร้างบัญชี และสร้างคีย์ API

2. รันคำสั่ง `/connect` และค้นหา **Venice AI**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Venice AI API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดลเช่น _Llama 3.3 70B_

   ```txt
   /models
   ```

---

### เกตเวย์ AI ของ Vercel

Vercel AI Gateway ช่วยให้คุณเข้าถึงโมเดลจาก OpenAI, Anthropic, Google, xAI และอื่นๆ อีกมากมายผ่านจุดสิ้นสุดแบบรวม โมเดลมีจำหน่ายในราคาปลีกโดยไม่มีส่วนเพิ่ม

1. ไปที่ [แดชบอร์ด Vercel](https://vercel.com/) ไปที่แท็บ **เกตเวย์ AI** และคลิก **API คีย์** เพื่อสร้างคีย์ API ใหม่

2. รันคำสั่ง `/connect` และค้นหา **Vercel AI Gateway**

   ```txt
   /connect
   ```

3. ป้อนคีย์ Vercel AI Gateway API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกรุ่น

   ```txt
   /models
   ```

คุณยังสามารถปรับแต่งโมเดลผ่านการกำหนดค่า opencode ของคุณได้อีกด้วย นี่คือตัวอย่างการระบุลำดับการกำหนดเส้นทางของผู้ให้บริการ

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "vercel": {
      "models": {
        "anthropic/claude-sonnet-4": {
          "options": {
            "order": ["anthropic", "vertex"]
          }
        }
      }
    }
  }
}
```

ตัวเลือกการกำหนดเส้นทางที่มีประโยชน์:

| ตัวเลือก            | คำอธิบาย                                                |
| ------------------- | ------------------------------------------------------- |
| `order`             | ลำดับผู้ให้บริการที่จะลอง                               |
| `only`              | จำกัดเฉพาะผู้ให้บริการบางราย                            |
| `zeroDataRetention` | ใช้ผู้ให้บริการที่ไม่มีนโยบายการเก็บรักษาข้อมูลเท่านั้น |

---

### xAI

1. ไปที่ [xAI console](https://console.x.ai/) สร้างบัญชี และสร้างคีย์ API

2. รันคำสั่ง `/connect` และค้นหา **xAI**

   ```txt
   /connect
   ```

3. ป้อนคีย์ xAI API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดลเช่น _Grok Beta_

   ```txt
   /models
   ```

---

### ซี.เอ.ไอ

1. ไปที่ [Z.AI API console](https://z.ai/manage-apikey/apikey-list) สร้างบัญชี และคลิก **สร้างคีย์ API ใหม่**

2. รันคำสั่ง `/connect` และค้นหา **Z.AI**

   ```txt
   /connect
   ```

   หากคุณสมัครสมาชิก **GLM Coding Plan** ให้เลือก **Z.AI Coding Plan**

3. ป้อนคีย์ Z.AI API ของคุณ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. รันคำสั่ง `/models` เพื่อเลือกโมเดลเช่น _GLM-4.7_

   ```txt
   /models
   ```

---

### ZenMux

1. ไปที่ [แดชบอร์ด ZenMux](https://zenmux.ai/settings/keys) คลิก **สร้าง API คีย์** และคัดลอกคีย์

2. รันคำสั่ง `/connect` และค้นหา ZenMux

   ```txt
   /connect
   ```

3. ป้อนคีย์ API สำหรับผู้ให้บริการ

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. ZenMux หลายรุ่นถูกโหลดไว้ล่วงหน้าตามค่าเริ่มต้น รันคำสั่ง `/models` เพื่อเลือกรุ่นที่คุณต้องการ

   ```txt
   /models
   ```

   คุณยังสามารถเพิ่มโมเดลเพิ่มเติมผ่านการกำหนดค่า opencode ของคุณ

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "zenmux": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

---

## ผู้ให้บริการที่กำหนดเอง

หากต้องการเพิ่มผู้ให้บริการ **รองรับ OpenAI** ที่ไม่อยู่ในคำสั่ง `/connect`:

:::tip
คุณสามารถใช้ผู้ให้บริการที่เข้ากันได้กับ OpenAI ด้วย opencode ผู้ให้บริการ AI สมัยใหม่ส่วนใหญ่เสนอ API ที่เข้ากันได้กับ OpenAI
:::

1. เรียกใช้คำสั่ง `/connect` และเลื่อนลงไปที่ **อื่นๆ**

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

2. ป้อนรหัสเฉพาะสำหรับผู้ให้บริการ

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◇  Enter provider id
   │  myprovider
   └
   ```

   :::note
   เลือก ID ที่น่าจดจำ คุณจะใช้รหัสนี้ในไฟล์กำหนดค่าของคุณ
   :::

3. ป้อนรหัส API ของคุณสำหรับผู้ให้บริการ

   ```bash
   $ /connect

   ┌  Add credential
   │
   ▲  This only stores a credential for myprovider - you will need to configure it in opencode.json, check the docs for examples.
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

4. สร้างหรืออัปเดตไฟล์ `opencode.json` ในไดเรกทอรีโครงการของคุณ:

   ```json title="opencode.json" ""myprovider"" {5-15}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "myprovider": {
         "npm": "@ai-sdk/openai-compatible",
         "name": "My AI ProviderDisplay Name",
         "options": {
           "baseURL": "https://api.myprovider.com/v1"
         },
         "models": {
           "my-model-name": {
             "name": "My Model Display Name"
           }
         }
       }
     }
   }
   ```

   นี่คือตัวเลือกการกำหนดค่า:
   - **npm**: แพ็คเกจ AI SDK ที่จะใช้ `@ai-sdk/openai-compatible` สำหรับผู้ให้บริการที่เข้ากันได้กับ OpenAI
   - **ชื่อ**: ชื่อที่แสดงใน UI
   - **รุ่น**: รุ่นที่มีจำหน่าย
   - **options.baseURL**: API ปลายทาง URL
   - **options.apiKey**: ตั้งค่าคีย์ API หรือไม่ก็ได้ หากไม่ได้ใช้การตรวจสอบสิทธิ์
   - **options.headers**: ตั้งค่าส่วนหัวที่กำหนดเองหรือไม่ก็ได้

   ข้อมูลเพิ่มเติมเกี่ยวกับตัวเลือกขั้นสูงในตัวอย่างด้านล่าง

5. รันคำสั่ง `/models` จากนั้นผู้ให้บริการและโมเดลที่คุณกำหนดเองจะปรากฏในรายการตัวเลือก

---

##### ตัวอย่าง

นี่คือตัวอย่างการตั้งค่าตัวเลือก `apiKey`, `headers` และโมเดล `limit`

```json title="opencode.json" {9,11,17-20}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "myprovider": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "My AI ProviderDisplay Name",
      "options": {
        "baseURL": "https://api.myprovider.com/v1",
        "apiKey": "{env:ANTHROPIC_API_KEY}",
        "headers": {
          "Authorization": "Bearer custom-token"
        }
      },
      "models": {
        "my-model-name": {
          "name": "My Model Display Name",
          "limit": {
            "context": 200000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

รายละเอียดการกำหนดค่า:

- **apiKey**: ตั้งค่าโดยใช้ไวยากรณ์ของตัวแปร `env` [เรียนรู้เพิ่มเติม](/docs/config#env-vars)
- **ส่วนหัว**: ส่วนหัวที่กำหนดเองส่งไปพร้อมกับคำขอแต่ละรายการ
- **limit.context**: Input Tokens สูงสุดที่โมเดลยอมรับ
- **limit.output**: Output Tokens สูงสุดที่โมเดลสามารถสร้างได้

ช่อง `limit` ช่วยให้ OpenCode เข้าใจบริบทที่คุณเหลืออยู่ ผู้ให้บริการมาตรฐานจะดึงข้อมูลเหล่านี้จาก models.dev โดยอัตโนมัติ

---

## การแก้ไขปัญหา

หากคุณประสบปัญหาในการกำหนดค่าผู้ให้บริการ ให้ตรวจสอบสิ่งต่อไปนี้:

1. **ตรวจสอบการตั้งค่าการรับรองความถูกต้อง**: เรียกใช้ `opencode auth list` เพื่อดูว่ามีข้อมูลรับรองหรือไม่
   สำหรับผู้ให้บริการจะถูกเพิ่มในการกำหนดค่าของคุณ

   สิ่งนี้ใช้ไม่ได้กับผู้ให้บริการอย่าง Amazon Bedrock ที่ต้องอาศัยตัวแปรสภาพแวดล้อมสำหรับการตรวจสอบสิทธิ์

2. สำหรับผู้ให้บริการแบบกำหนดเอง ให้ตรวจสอบการกำหนดค่า opencode และ:
   - ตรวจสอบให้แน่ใจว่า ID ผู้ให้บริการที่ใช้ในคำสั่ง `/connect` ตรงกับ ID ในการกำหนดค่า opencode ของคุณ
   - มีการใช้แพ็คเกจ npm ที่ถูกต้องสำหรับผู้ให้บริการ ตัวอย่างเช่น ใช้ `@ai-sdk/cerebras` สำหรับ Cerebras และสำหรับผู้ให้บริการรายอื่นที่เข้ากันได้กับ OpenAI ให้ใช้ `@ai-sdk/openai-compatible`
   - ตรวจสอบว่ามีการใช้จุดสิ้นสุด API ที่ถูกต้องในช่อง `options.baseURL`
